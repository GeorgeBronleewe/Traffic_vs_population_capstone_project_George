{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ac87b-489d-4549-b81a-bf712bb558d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee45cd8-87c7-4bdc-b8c1-4f2fd8c3c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = r\"Traffic_Lines_-5158403106151148634.csv\"\n",
    "GEO_PATH = r\"Traffic_Lines_-8205076846245197459.geojson\"\n",
    "\n",
    "CSV_ID_COL = \"OBJECTID\"\n",
    "GEO_ID_COL = \"OBJECTID\"\n",
    "\n",
    "OUT_BASENAME = Path(\"aadt_davidson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6666b-df4e-4701-afed-1a5a428028be",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = CSV_PATH if \"CSV_PATH\" in globals() else \"Traffic_Lines_-5158403106151148634.csv\"\n",
    "OUT_BASENAME = OUT_BASENAME if \"OUT_BASENAME\" in globals() else Path(\"aadt_davidson\")\n",
    "\n",
    "YEAR_ANY_RE = re.compile(r\"(?:^|[^0-9])((?:19|20)\\d{2})(?:[^0-9]|$)\", flags=re.IGNORECASE)\n",
    "\n",
    "def _find_long_year_col(df: pd.DataFrame) -> str | None:\n",
    "    \"\"\"returns the column that stores the year (prefers name-based, then value-based)\"\"\"\n",
    "    for c in df.columns:\n",
    "        s = str(c).strip().lower()\n",
    "        if s in {\"year\", \"yr\"} or \"year\" in s:\n",
    "            return c\n",
    "    best, best_share = None, -1.0\n",
    "    for c in df.columns:\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        share = s.between(1990, 2035).mean()\n",
    "        if share > best_share:\n",
    "            best, best_share = c, share\n",
    "    return best if best_share >= 0.50 else None\n",
    "\n",
    "def _find_long_aadt_col(df: pd.DataFrame, year_col: str | None) -> str | None:\n",
    "    \"\"\"returns the column that stores AADT (excludes the detected year column)\"\"\"\n",
    "    def _ok(c):\n",
    "        return c != year_col\n",
    "    # prefers explicit names\n",
    "    for c in df.columns:\n",
    "        if _ok(c) and \"aadt\" in str(c).lower() and \"year\" not in str(c).lower():\n",
    "            return c\n",
    "    # falls back to numeric-dense column with range, excluding year_col\n",
    "    best, best_density, best_range = None, -1.0, -1.0\n",
    "    for c in df.columns:\n",
    "        if not _ok(c):\n",
    "            continue\n",
    "        v = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        dens = v.notna().mean()\n",
    "        rng = (v.max() - v.min()) if v.notna().any() else -1\n",
    "        if dens > best_density or (dens == best_density and rng > best_range):\n",
    "            best, best_density, best_range = c, dens, rng\n",
    "    return best if best_density >= 0.50 else None\n",
    "\n",
    "def _detect_wide_year_cols(df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"returns columns that contain a 4-digit year in their name\"\"\"\n",
    "    cols = []\n",
    "    for c in df.columns:\n",
    "        if YEAR_ANY_RE.search(str(c)):\n",
    "            cols.append(c)\n",
    "    bad_tokens = {\"objectid\",\"route id\",\"route_id\",\"gisid\",\"segment\",\"station\",\"count_station\"}\n",
    "    return [c for c in cols if not any(tok in str(c).lower() for tok in bad_tokens)]\n",
    "\n",
    "def _extract_year_from_name(colname: str) -> int:\n",
    "    \"\"\"extracts the 4-digit year from a column name\"\"\"\n",
    "    m = YEAR_ANY_RE.search(str(colname))\n",
    "    if not m:\n",
    "        raise ValueError(f\"Cannot parse year from column: {colname}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "\n",
    "year_col = _find_long_year_col(df)\n",
    "aadt_col = _find_long_aadt_col(df, year_col)\n",
    "wide_year_cols = _detect_wide_year_cols(df)\n",
    "\n",
    "print(\"Detected year_col:\", year_col)\n",
    "print(\"Detected aadt_col:\", aadt_col)\n",
    "print(\"Wide year-like columns (sample):\", wide_year_cols[:10])\n",
    "\n",
    "# builds long_df\n",
    "if year_col is not None and aadt_col is not None:\n",
    "    # treats file as already long format\n",
    "    year_s = pd.to_numeric(df[year_col], errors=\"coerce\")\n",
    "    aadt_s = pd.to_numeric(df[aadt_col], errors=\"coerce\")\n",
    "    long_df = pd.DataFrame({\"year\": year_s, \"aadt\": aadt_s}).dropna(subset=[\"year\",\"aadt\"])\n",
    "else:\n",
    "    # treats as wide if possible\n",
    "    if not wide_year_cols:\n",
    "        raise ValueError(\n",
    "            \"No suitable AADT column and no year-like header columns found.\\n\"\n",
    "            f\"Columns (first 25): {list(df.columns)[:25]}\"\n",
    "        )\n",
    "    melted = df.melt(\n",
    "        id_vars=[c for c in df.columns if c not in wide_year_cols],\n",
    "        value_vars=wide_year_cols,\n",
    "        var_name=\"year_raw\",\n",
    "        value_name=\"aadt\"\n",
    "    )\n",
    "    long_df = melted.assign(\n",
    "        year=lambda d: d[\"year_raw\"].map(_extract_year_from_name),\n",
    "        aadt=lambda d: pd.to_numeric(d[\"aadt\"], errors=\"coerce\")\n",
    "    ).dropna(subset=[\"year\",\"aadt\"])[[\"year\",\"aadt\"]]\n",
    "\n",
    "# aggregates to calendar-year totals\n",
    "traffic_year = (\n",
    "    long_df.groupby(\"year\", as_index=False)[\"aadt\"].sum()\n",
    "           .rename(columns={\"aadt\":\"aadt_total\"})\n",
    "           .sort_values(\"year\")\n",
    "           .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "out_dir = Path(\"outputs\"); out_dir.mkdir(exist_ok=True)\n",
    "tidy_path = out_dir / \"aadt_davidson_long.csv\"\n",
    "year_path = out_dir / \"aadt_davidson_year_totals.csv\"\n",
    "long_df[[\"year\",\"aadt\"]].to_csv(tidy_path, index=False)\n",
    "traffic_year.to_csv(year_path, index=False)\n",
    "\n",
    "print(\"Years present:\", traffic_year[\"year\"].astype(int).tolist())\n",
    "display(traffic_year.head(10)); display(traffic_year.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dab138b-d52c-4d2a-a486-761ea707774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Coverage check: verifies needed years for ACS alignment (2013–2023) ===\n",
    "need = list(range(2013, 2024))\n",
    "have = set(traffic_year[\"year\"].astype(int))\n",
    "missing = sorted([y for y in need if y not in have])\n",
    "\n",
    "print(\"Years present:\", sorted(have))\n",
    "print(\"Years missing (need 2013–2023):\", missing)\n",
    "\n",
    "# writes a small report\n",
    "import pandas as pd, numpy as np\n",
    "pd.DataFrame({\"year\": need, \"present\": [y in have for y in need]}) \\\n",
    "  .to_csv(Path(\"outputs\") / \"aadt_needed_years_2013_2023.csv\", index=False)\n",
    "print(\"Wrote:\", Path(\"outputs\") / \"aadt_needed_years_2013_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942525d7-558f-480b-b4dc-9cfbea724012",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_ID_COLS = [\n",
    "    \"STATION\",\"STATION_ID\",\"STATIONID\",\"COUNT_STATION\",\"COUNT_STATION_ID\",\n",
    "    \"LOC_ID\",\"LOCATION_ID\",\"SITE_NO\",\"SITENO\",\"POINT_ID\",\"POINTID\",\n",
    "    \"SEGMENT_ID\",\"SEGMENTID\",\"GISID\",\"GIS_ID\",\"ID\",\"OBJECTID\"\n",
    "]\n",
    "\n",
    "def _auto_id(cols):\n",
    "    low = {c.lower(): c for c in cols}\n",
    "    for cand in CANDIDATE_ID_COLS:\n",
    "        if cand.lower() in low:\n",
    "            return low[cand.lower()]\n",
    "    return None\n",
    "\n",
    "def _norm_id(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str).str.strip()\n",
    "    s = s.str.replace(r\"\\s+\", \"\", regex=True)\n",
    "    s = s.str.upper()\n",
    "    s = s.str.replace(r\"^STATION[_\\-]?\", \"\", regex=True)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9256c34-56be-4757-a7a1-4c9b770bccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR_COL_RE = re.compile(r\"^(?:AADT[_\\-\\s]*)?(19|20)\\d{2}$\")\n",
    "\n",
    "def _detect_year_cols(df: pd.DataFrame):\n",
    "    yc = [c for c in df.columns if YEAR_COL_RE.match(str(c).strip())]\n",
    "    if not yc:\n",
    "        yc = [c for c in df.columns if re.fullmatch(r\"(19|20)\\d{2}\", str(c).strip())]\n",
    "    return yc\n",
    "\n",
    "def _year_from(colname: str) -> int:\n",
    "    m = re.search(r\"(19|20)\\d{2}\", str(colname))\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not parse year from column: {colname}\")\n",
    "    return int(m.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943ccfc-9e7a-425a-8dc9-6344795a6d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tidy_from_wide(df: pd.DataFrame, id_col: str, keep_cols=None) -> pd.DataFrame:\n",
    "    keep_cols = keep_cols or []\n",
    "    year_cols = _detect_year_cols(df)\n",
    "\n",
    "    # Tall format (e.g., 'AADT Year' + 'AADT')\n",
    "    if not year_cols:\n",
    "        year_candidates = [c for c in df.columns if \"year\" in str(c).lower()]\n",
    "        year_col = next((c for c in [\"year\",\"Year\"] + year_candidates if c in df.columns), None)\n",
    "        aadt_col = next((c for c in [\"AADT\",\"aadt\",\"AADT_TOTAL\",\"TOTAL_AADT\",\"value\"] if c in df.columns), None)\n",
    "        if not (year_col and aadt_col):\n",
    "            raise ValueError(\"Need a year-like column and an AADT column.\")\n",
    "        out = df[[id_col, year_col, aadt_col] + [c for c in keep_cols if c in df.columns]].copy()\n",
    "        out = out.rename(columns={year_col: \"year\", aadt_col: \"aadt\"})\n",
    "        out[\"year\"] = pd.to_numeric(out[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        out[\"aadt\"] = pd.to_numeric(out[\"aadt\"], errors=\"coerce\")\n",
    "        return out\n",
    "\n",
    "    # Wide format (year columns across)\n",
    "    long_df = df.melt(\n",
    "        id_vars=[c for c in df.columns if c not in year_cols],\n",
    "        value_vars=year_cols, var_name=\"year_raw\", value_name=\"aadt\"\n",
    "    )\n",
    "    long_df[\"year\"] = long_df[\"year_raw\"].apply(_year_from)\n",
    "    long_df = long_df.drop(columns=[\"year_raw\"])\n",
    "    keep = [id_col, \"year\", \"aadt\"] + [c for c in keep_cols if c in long_df.columns]\n",
    "    long_df = long_df[keep].copy()\n",
    "    long_df[\"aadt\"] = pd.to_numeric(long_df[\"aadt\"], errors=\"coerce\")\n",
    "    return long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe6531f-ea51-48b6-b48d-868c750a35e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_head = pd.read_csv(CSV_PATH, nrows=5)\n",
    "print(\"CSV columns:\", sorted(df_head.columns.tolist()))\n",
    "csv_ids = [c for c in df_head.columns if c.upper() in CANDIDATE_ID_COLS]\n",
    "print(\"Likely CSV IDs:\", csv_ids)\n",
    "\n",
    "gdf_head = gpd.read_file(GEO_PATH).head(5)\n",
    "print(\"GEO columns:\", sorted(gdf_head.columns.tolist()))\n",
    "geo_ids = [c for c in gdf_head.columns if c.upper() in CANDIDATE_ID_COLS]\n",
    "print(\"Likely GEO IDs:\", geo_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6cc7ac-5047-408c-a304-2960c72d6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "df[CSV_ID_COL] = _norm_id(df[CSV_ID_COL])\n",
    "\n",
    "keep_cols = [c for c in [\"Route ID\",\"ROUTE\",\"ROAD_NAME\",\"ROUTE_NAME\",\"ROADWAY\",\"DIR\",\"DIRECTION\"] if c in df.columns]\n",
    "tidy = (_tidy_from_wide(df, id_col=CSV_ID_COL, keep_cols=keep_cols)\n",
    "        .rename(columns={CSV_ID_COL: \"station_id\"}))\n",
    "tidy = tidy[tidy[\"aadt\"].notna()].copy()\n",
    "\n",
    "OUT_BASENAME.parent.mkdir(parents=True, exist_ok=True)\n",
    "tidy_out = OUT_BASENAME.with_suffix(\".clean.csv\")\n",
    "tidy.to_csv(tidy_out, index=False)\n",
    "\n",
    "print(\"Saved:\", tidy_out, \"| rows:\", len(tidy))\n",
    "tidy.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7113a4-f7f3-4e09-8695-d706b320ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_ids = set(tidy[\"station_id\"].unique())\n",
    "\n",
    "gdf = gpd.read_file(GEO_PATH)\n",
    "gdf = gdf[[GEO_ID_COL, \"geometry\"]]  # drop extras early\n",
    "gdf[GEO_ID_COL] = _norm_id(gdf[GEO_ID_COL])\n",
    "gdf = gdf[gdf[GEO_ID_COL].isin(needed_ids)].drop_duplicates(subset=[GEO_ID_COL])\n",
    "\n",
    "print(\"Geometry rows after filter:\", len(gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97445c47-4a1b-4449-860a-8f63c43164cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gdf.crs is None:\n",
    "    gdf = gdf.set_crs(epsg=4326)\n",
    "elif getattr(gdf.crs, \"to_epsg\", lambda: None)() != 4326:\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "gdf = gdf.rename(columns={GEO_ID_COL: \"station_id\"})\n",
    "merged = gdf.merge(tidy, on=\"station_id\", how=\"inner\", sort=False)\n",
    "\n",
    "print(\"Merged rows:\", len(merged))\n",
    "merged.head(3).drop(columns=\"geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd3521-8c0a-42d9-827a-8dd9643e4f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_geojson = OUT_BASENAME.with_suffix(\".geojson\")\n",
    "merged.to_file(out_geojson, driver=\"GeoJSON\")\n",
    "print(\"Wrote:\", out_geojson)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a0c37c-efb2-462c-bcc8-b9af5e255045",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_counts = tidy.groupby(\"year\").size().rename(\"rows\").reset_index().sort_values(\"year\")\n",
    "year_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af691c-63d3-434f-87cc-5988e6423280",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy.sort_values(\"aadt\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2bcac-c2cd-4ce0-b219-975be2a0a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_years = tidy[~tidy[\"year\"].between(1990, 2025)][\"year\"].unique()\n",
    "print(\"Out-of-range years:\", bad_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a6605-6403-4a4e-9a50-f329d2bda4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy[\"year\"] = tidy[\"year\"].astype(int)\n",
    "\n",
    "# Now group and plot\n",
    "totals = tidy.groupby(\"year\", as_index=False)[\"aadt\"].sum()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(totals[\"year\"], totals[\"aadt\"], marker=\"o\")\n",
    "plt.title(\"Total AADT across stations by year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Sum of AADT\")\n",
    "plt.xticks(totals[\"year\"])  # show only the years present\n",
    "plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efb7a4-f419-4ed6-bfa1-bb28246c1210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only plausible years (e.g., 1990–2025)\n",
    "totals = (\n",
    "    tidy[tidy[\"year\"].between(1990, 2025)]\n",
    "    .groupby(\"year\", as_index=False)[\"aadt\"]\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(totals[\"year\"], totals[\"aadt\"], marker=\"o\")\n",
    "plt.title(\"Total AADT across stations by year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Sum of AADT\")\n",
    "plt.tight_layout()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63e87d-90b7-45e0-a5a9-5b5ba54dbd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_station = tidy[\"station_id\"].value_counts().idxmax()\n",
    "s = tidy[tidy[\"station_id\"] == top_station].sort_values(\"year\")\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(s[\"year\"], s[\"aadt\"])\n",
    "plt.title(f\"AADT trend — station {top_station}\")\n",
    "plt.xlabel(\"Year\"); plt.ylabel(\"AADT\")\n",
    "plt.tight_layout(); #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cca5ee-be81-4179-b0be-eaeeb7a2535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required fields & aliases\n",
    "fields = [\"station_id\", \"year\", \"aadt\"]\n",
    "aliases = [\"Station\", \"Year\", \"AADT\"]\n",
    "\n",
    "# Optional fields (check for existence in merged)\n",
    "optional = [\n",
    "    (\"Route ID\", \"Route ID\"),\n",
    "    (\"ROUTE\", \"ROUTE\"),\n",
    "    (\"ROAD_NAME\", \"Road Name\"),\n",
    "    (\"ROUTE_NAME\", \"Route Name\"),\n",
    "    (\"ROADWAY\", \"Roadway\")\n",
    "]\n",
    "\n",
    "for col, alias in optional:\n",
    "    if col in merged.columns:\n",
    "        fields.append(col)\n",
    "        aliases.append(alias)\n",
    "\n",
    "# Now both lists match in length\n",
    "m = folium.Map(location=[36.1627, -86.7816], zoom_start=10)\n",
    "folium.GeoJson(\n",
    "    merged.to_json(),\n",
    "    name=\"AADT\",\n",
    "    tooltip=folium.features.GeoJsonTooltip(\n",
    "        fields=fields,\n",
    "        aliases=aliases,\n",
    "        sticky=False\n",
    "    ),\n",
    ").add_to(m)\n",
    "folium.LayerControl().add_to(m)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3979a0-8cbd-4ffa-af10-58f386007116",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy[\"year\"] = tidy[\"year\"].astype(int)\n",
    "print(\"Unique years in tidy:\", sorted(tidy[\"year\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06692a60-b4cb-412a-8feb-45942e25f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "topN = 25  # change as needed\n",
    "top_segments = (\n",
    "    tidy.sort_values(\"aadt\", ascending=False)\n",
    "        .head(topN)\n",
    "        .copy()\n",
    ")\n",
    "\n",
    "top_segments[[\"station_id\",\"year\",\"aadt\"] + [c for c in [\"Route ID\",\"ROUTE\",\"ROAD_NAME\",\"ROUTE_NAME\",\"ROADWAY\"] if c in top_segments.columns]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f957a134-a4f0-476d-b6eb-a6f16dfeaadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,10))\n",
    "plt.barh(\n",
    "    top_segments[\"station_id\"].astype(str)[::-1],\n",
    "    top_segments[\"aadt\"][::-1]\n",
    ")\n",
    "plt.title(\"Top AADT Segments — 2024\")\n",
    "plt.xlabel(\"AADT\")\n",
    "plt.ylabel(\"Station ID\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c64dd7-bd28-46cb-9773-f206c8ba87a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_cols = [c for c in [\"Route ID\",\"ROUTE\",\"ROUTE_NAME\",\"ROAD_NAME\",\"ROADWAY\"] if c in tidy.columns]\n",
    "agg_cols = [\"aadt\"]\n",
    "if \"VMT\" in tidy.columns:\n",
    "    agg_cols.append(\"VMT\")\n",
    "\n",
    "by_route = (\n",
    "    tidy.groupby(route_cols, dropna=False)[agg_cols]\n",
    "        .sum(numeric_only=True)\n",
    "        .reset_index()\n",
    "        .sort_values(\"aadt\", ascending=False)\n",
    ")\n",
    "\n",
    "by_route.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184754bf-888a-459a-861a-858d7a763446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you've already created `merged`, it’s 2024-only.\n",
    "# If not, run your earlier merge steps first.\n",
    "print(\"Merged rows (2024 only):\", len(merged))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07bc8be-bfeb-443a-9e9b-b64267ef2dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required tooltip fields\n",
    "fields = [\"station_id\", \"aadt\"]\n",
    "aliases = [\"Station\", \"AADT\"]\n",
    "\n",
    "# Optional details if present\n",
    "optional = [\n",
    "    (\"Route ID\", \"Route ID\"),\n",
    "    (\"ROUTE\", \"ROUTE\"),\n",
    "    (\"ROAD_NAME\", \"Road Name\"),\n",
    "    (\"ROUTE_NAME\", \"Route Name\"),\n",
    "    (\"ROADWAY\", \"Roadway\"),\n",
    "    (\"VMT\", \"VMT\")\n",
    "]\n",
    "for col, alias in optional:\n",
    "    if col in merged.columns:\n",
    "        fields.append(col)\n",
    "        aliases.append(alias)\n",
    "\n",
    "m = folium.Map(location=[36.1627, -86.7816], zoom_start=10)\n",
    "\n",
    "# Simple style by quantiles of AADT\n",
    "try:\n",
    "    q = merged[\"aadt\"].quantile([0, 0.5, 0.75, 0.9, 0.99, 1]).tolist()\n",
    "    def style_fn(feature):\n",
    "        a = feature[\"properties\"].get(\"aadt\", 0) or 0\n",
    "        if a <= q[1]: w = 2\n",
    "        elif a <= q[2]: w = 3\n",
    "        elif a <= q[3]: w = 4\n",
    "        elif a <= q[4]: w = 5\n",
    "        else: w = 6\n",
    "        return {\"weight\": w}\n",
    "except Exception:\n",
    "    def style_fn(_): return {\"weight\": 3}\n",
    "\n",
    "folium.GeoJson(\n",
    "    merged.to_json(),\n",
    "    name=\"AADT 2024\",\n",
    "    style_function=style_fn,\n",
    "    tooltip=folium.features.GeoJsonTooltip(fields=fields, aliases=aliases, sticky=False),\n",
    ").add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00577d6-6f1a-4f09-a1f2-2219900d862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment-level 2024\n",
    "seg2024 = merged.drop(columns=\"geometry\")\n",
    "seg2024_out = OUT_BASENAME.with_suffix(\".segments_2024.csv\")\n",
    "seg2024.to_csv(seg2024_out, index=False)\n",
    "print(\"Wrote:\", seg2024_out)\n",
    "\n",
    "# Route-level 2024 (from by_route above)\n",
    "route2024_out = OUT_BASENAME.with_suffix(\".routes_2024.csv\")\n",
    "by_route.to_csv(route2024_out, index=False)\n",
    "print(\"Wrote:\", route2024_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3428bf-b122-4476-b4d3-432e0b779ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuses CSV_PATH and OUT_BASENAME if already defined; otherwise sets defaults\n",
    "CSV_PATH = CSV_PATH if \"CSV_PATH\" in globals() else \"Traffic_Lines_-5158403106151148634.csv\"\n",
    "OUT_BASENAME = OUT_BASENAME if \"OUT_BASENAME\" in globals() else Path(\"aadt_davidson\")\n",
    "\n",
    "# defines year-column detector if missing\n",
    "if \"_detect_year_cols\" not in globals():\n",
    "    YEAR_COL_RE = re.compile(r\"^(?:AADT[_\\-\\s]*)?(19|20)\\d{2}$\")\n",
    "    def _detect_year_cols(df: pd.DataFrame):\n",
    "        yc = [c for c in df.columns if YEAR_COL_RE.match(str(c).strip())]\n",
    "        if not yc:\n",
    "            yc = [c for c in df.columns if re.fullmatch(r\"(19|20)\\d{2}\", str(c).strip())]\n",
    "        return yc\n",
    "    def _year_from(colname: str) -> int:\n",
    "        m = re.search(r\"(19|20)\\d{2}\", str(colname))\n",
    "        if not m:\n",
    "            raise ValueError(f\"Could not parse year from column: {colname}\")\n",
    "        return int(m.group(0))\n",
    "\n",
    "df_head = pd.read_csv(CSV_PATH, nrows=5, low_memory=False)\n",
    "print(\"CSV columns (sample):\", sorted(df_head.columns.tolist())[:25])\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "year_cols = _detect_year_cols(df)\n",
    "years_parsed = sorted({_year_from(c) for c in year_cols})\n",
    "\n",
    "print(\"\\nDetected year columns:\", year_cols[:20], (\"...\" if len(year_cols) > 20 else \"\"))\n",
    "print(\"Parsed years:\", years_parsed)\n",
    "\n",
    "# builds year coverage summary\n",
    "summary = []\n",
    "for c in year_cols:\n",
    "    y = _year_from(c)\n",
    "    s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    summary.append({\n",
    "        \"year\": y,\n",
    "        \"non_null_rows\": int(s.notna().sum()),\n",
    "        \"min_aadt\": float(np.nanmin(s)) if s.notna().any() else np.nan,\n",
    "        \"median_aadt\": float(np.nanmedian(s)) if s.notna().any() else np.nan,\n",
    "        \"max_aadt\": float(np.nanmax(s)) if s.notna().any() else np.nan\n",
    "    })\n",
    "years_summary = pd.DataFrame(summary).sort_values(\"year\")\n",
    "display(years_summary)\n",
    "\n",
    "OUT_BASENAME.parent.mkdir(parents=True, exist_ok=True)\n",
    "years_summary.to_csv(OUT_BASENAME.with_suffix(\".years_summary.csv\"), index=False)\n",
    "print(\"Wrote:\", OUT_BASENAME.with_suffix(\".years_summary.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66617a90-1189-4aa2-900a-4ad01091744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tidy_from_wide(df: pd.DataFrame, id_col: str | None = None, keep_cols=None) -> pd.DataFrame:\n",
    "    \"\"\"melts wide year columns into long format and extracts numeric year\"\"\"\n",
    "    keep_cols = keep_cols or []\n",
    "    ycols = _detect_year_cols(df)\n",
    "    if not ycols:\n",
    "        raise ValueError(\"No year-like columns found.\")\n",
    "    long_df = df.melt(\n",
    "        id_vars=[c for c in df.columns if c not in ycols],\n",
    "        value_vars=ycols, var_name=\"year_raw\", value_name=\"aadt\"\n",
    "    )\n",
    "    long_df[\"year\"] = long_df[\"year_raw\"].map(lambda s: _year_from(s))\n",
    "    cols = ([\"year\", \"aadt\"] + [c for c in keep_cols if c in long_df.columns])\n",
    "    out = long_df[cols].copy()\n",
    "    out[\"aadt\"] = pd.to_numeric(out[\"aadt\"], errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "# chooses optional keep columns if present\n",
    "keep_cols = [c for c in [\"Route ID\",\"ROUTE\",\"ROAD_NAME\",\"ROUTE_NAME\",\"ROADWAY\",\"DIR\",\"DIRECTION\",\"COUNTY\",\"COUNTY_NAME\"] if c in df.columns]\n",
    "tidy = _tidy_from_wide(df, keep_cols=keep_cols).dropna(subset=[\"aadt\"])\n",
    "\n",
    "# aggregates to year; filters to target window if desired\n",
    "traffic_year = (\n",
    "    tidy.groupby(\"year\", as_index=False)[\"aadt\"].sum()\n",
    "        .rename(columns={\"aadt\": \"aadt_total\"})\n",
    "        .sort_values(\"year\")\n",
    ")\n",
    "display(traffic_year.head(10))\n",
    "display(traffic_year.tail(10))\n",
    "\n",
    "# writes canonical tidy and aggregated outputs for Capstone3\n",
    "out_dir = Path(\"outputs\"); out_dir.mkdir(exist_ok=True)\n",
    "tidy_path = out_dir / \"aadt_davidson_long.csv\"\n",
    "year_path = out_dir / \"aadt_davidson_year_totals.csv\"\n",
    "\n",
    "tidy.to_csv(tidy_path, index=False)\n",
    "traffic_year.to_csv(year_path, index=False)\n",
    "print(\"Wrote:\", tidy_path)\n",
    "print(\"Wrote:\", year_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411361ec-5958-478c-8698-94332aca2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2013–2023 readiness check: verifies coverage needed for ACS 5-year periods ===\n",
    "need_years = list(range(2013, 2024))  # inclusive 2013..2023\n",
    "have_years = set(map(int, traffic_year[\"year\"].tolist()))\n",
    "missing = sorted([y for y in need_years if y not in have_years])\n",
    "\n",
    "print(\"Years needed (2013–2023):\", need_years)\n",
    "print(\"Years present:\", sorted(have_years))\n",
    "print(\"Years missing:\", missing)\n",
    "\n",
    "# saves a small report\n",
    "pd.DataFrame({\"need_year\": need_years, \"present\": [y in have_years for y in need_years]}) \\\n",
    "  .to_csv(Path(\"outputs\") / \"aadt_needed_years_2013_2023.csv\", index=False)\n",
    "print(\"Wrote:\", Path(\"outputs\") / \"aadt_needed_years_2013_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6352e1b7-71a0-40b7-beb3-0e1d4289d920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a8fa67-9670-46cc-b605-0bbcbd5329ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1625f79e-4096-4d05-8013-cb5fbf43bd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ce137-53f1-4892-bca9-f71c04d205a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc910682-f4cb-47d9-89da-3824e311b61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd107a-b006-4a35-947f-cbc5ecb6fc21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
