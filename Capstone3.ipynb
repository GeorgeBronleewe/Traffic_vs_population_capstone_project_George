{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58eb75f-ee04-4ec3-aa8f-ab1887e302e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d20be-0729-49ec-9af0-1b05c3cb3754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines canonical folders\n",
    "OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "FIG_DIR = Path(\"figures\"); FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# traffic totals produced by Data Check.ipynb\n",
    "TRAFFIC_TOTALS_CSV = OUT_DIR / \"aadt_davidson_year_totals.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758625d5-d040-4605-983f-831d08667e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads year totals, normalizes headers, validates coverage for 2013–2023\n",
    "assert TRAFFIC_TOTALS_CSV.exists(), (\n",
    "    f\"Missing {TRAFFIC_TOTALS_CSV}. Open Data Check.ipynb and run either downloader first.\"\n",
    ")\n",
    "\n",
    "t = pd.read_csv(TRAFFIC_TOTALS_CSV)\n",
    "\n",
    "# normalizes expected columns\n",
    "if \"AADT_YEAR\" in t.columns and \"year\" not in t.columns:\n",
    "    t = t.rename(columns={\"AADT_YEAR\": \"year\"})\n",
    "if \"aadt_total\" not in t.columns and \"AADT\" in t.columns:\n",
    "    t = (t.groupby(\"year\", as_index=False)[\"AADT\"]\n",
    "           .sum()\n",
    "           .rename(columns={\"AADT\": \"aadt_total\"}))\n",
    "\n",
    "traffic_year = (\n",
    "    t.assign(year=pd.to_numeric(t[\"year\"], errors=\"coerce\"),\n",
    "             aadt_total=pd.to_numeric(t[\"aadt_total\"], errors=\"coerce\"))\n",
    "     .dropna(subset=[\"year\",\"aadt_total\"])\n",
    "     .assign(year=lambda d: d[\"year\"].round().astype(int))\n",
    "     .query(\"1990 <= year <= 2035\")\n",
    "     .drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
    "     .sort_values(\"year\")\n",
    "     .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "years = traffic_year[\"year\"].tolist()\n",
    "missing = [y for y in range(2013, 2024) if y not in set(years)]\n",
    "if missing:\n",
    "    raise ValueError(\n",
    "        f\"Traffic totals missing years {missing}. \"\n",
    "        \"Re-run the RAW POINTS (paged) downloader in Data Check.ipynb.\"\n",
    "    )\n",
    "\n",
    "print(\"traffic year coverage:\", years[0], \"→\", years[-1], \"| distinct years:\", len(years))\n",
    "display(traffic_year.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36afe5be-d248-4051-80ca-eed61b5ca678",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"traffic years:\", traffic_year[\"year\"].min(), \"→\", traffic_year[\"year\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac813027-e914-42ee-9fba-a4357cfd2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds pop_clean = [['year','population_total']] using local CSVs first, then fills missing years from ACS (2014–2023)\n",
    "SEARCH_DIRS = [Path(\".\"), Path(\"data\"), Path(\"data_raw\"), Path(\"outputs\")]\n",
    "TARGET_YEARS = list(range(2014, 2024))               # ACS 5-year end-years we want\n",
    "GEOID_DAVIDSON = {\"state\": \"47\", \"county\": \"037\"}    # Tennessee / Davidson\n",
    "\n",
    "def _read_csv_safe(p: Path):\n",
    "    try:\n",
    "        return pd.read_csv(p, low_memory=False)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.read_csv(p, encoding=\"latin-1\", low_memory=False)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def _guess_year_from_filename(p: Path) -> int | None:\n",
    "    m = re.search(r\"(20\\d{2}|19\\d{2})\", p.stem)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def _find_pop_col(df: pd.DataFrame) -> str | None:\n",
    "    # prefer standard ACS total-pop columns\n",
    "    preferred = [\"B01003_001E\", \"DP05_0001E\", \"S0101_C01_001E\"]\n",
    "    for c in df.columns:\n",
    "        if c in preferred: \n",
    "            return c\n",
    "    # otherwise any column with 'pop' in the name, else widest numeric column\n",
    "    poplike = [c for c in df.columns if \"pop\" in str(c).lower()]\n",
    "    if poplike: \n",
    "        return poplike[0]\n",
    "    num = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return num[0] if num else None\n",
    "\n",
    "def _select_davidson(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_cols = {c.lower(): c for c in df.columns}\n",
    "    # by GEOID 47037 (often appears as 'state','county' OR 'GEO_ID'/'GEOID')\n",
    "    if \"state\" in df_cols and \"county\" in df_cols:\n",
    "        st, co = df_cols[\"state\"], df_cols[\"county\"]\n",
    "        return df[(df[st].astype(str)==GEOID_DAVIDSON[\"state\"]) & (df[co].astype(str)==GEOID_DAVIDSON[\"county\"])]\n",
    "    for key in (\"GEOID\",\"GEO_ID\",\"geoid\",\"geo_id\"):\n",
    "        if key in df.columns:\n",
    "            mask = df[key].astype(str).str.contains(\"47037\", na=False)\n",
    "            sub = df[mask]\n",
    "            if not sub.empty: return sub\n",
    "    if \"NAME\" in df.columns:\n",
    "        mask = df[\"NAME\"].astype(str).str.contains(\"Davidson County\", case=False, na=False) & \\\n",
    "               df[\"NAME\"].astype(str).str.contains(\"Tennessee\", case=False, na=False)\n",
    "        sub = df[mask]\n",
    "        if not sub.empty: return sub\n",
    "    # last resort: return as-is\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700644b-b760-4a6e-810e-06531eaa5b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_year_row(p: Path, df: pd.DataFrame) -> dict | None:\n",
    "    df = _select_davidson(df.copy())\n",
    "    if df.empty: \n",
    "        return None\n",
    "    pop_col = _find_pop_col(df)\n",
    "    if pop_col is None: \n",
    "        return None\n",
    "    year = None\n",
    "    # try explicit year column\n",
    "    for cand in df.columns:\n",
    "        if cand.lower() in (\"year\",\"end_year\",\"acs_year\"):\n",
    "            v = pd.to_numeric(df[cand], errors=\"coerce\").dropna()\n",
    "            if not v.empty and (1990 <= int(v.iloc[0]) <= 2035):\n",
    "                year = int(v.iloc[0]); break\n",
    "    # try filename year\n",
    "    if year is None:\n",
    "        year = _guess_year_from_filename(p)\n",
    "    # try NAME like \"... 2018 5-year\"\n",
    "    if year is None and \"NAME\" in df.columns:\n",
    "        m = re.search(r\"(20\\d{2}|19\\d{2})\", \" \".join(df[\"NAME\"].astype(str).head(3).tolist()))\n",
    "        if m: year = int(m.group(1))\n",
    "    if year is None: \n",
    "        return None\n",
    "    val = pd.to_numeric(df[pop_col], errors=\"coerce\").dropna()\n",
    "    if val.empty: \n",
    "        return None\n",
    "    return {\"year\": year, \"population_total\": int(round(val.iloc[0]))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ea94d0-4ad6-424d-bc3c-ffe8815daa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_rows = []\n",
    "candidates = []\n",
    "for base in SEARCH_DIRS:\n",
    "    if base.exists():\n",
    "        for p in base.rglob(\"*.csv\"):\n",
    "            # prioritize likely ACS files (your screenshot shows many \"USC...\" files too)\n",
    "            if any(tok in p.name.lower() for tok in [\"acs\",\"dp05\",\"b01003\",\"population\",\"usc\"]):\n",
    "                candidates.append(p)\n",
    "\n",
    "seen_years = set()\n",
    "for p in sorted(set(candidates)):\n",
    "    df = _read_csv_safe(p)\n",
    "    if df is None: \n",
    "        continue\n",
    "    rec = _extract_year_row(p, df)\n",
    "    if rec and rec[\"year\"] in range(2010, 2036) and rec[\"year\"] not in seen_years:\n",
    "        local_rows.append(rec); seen_years.add(rec[\"year\"])\n",
    "\n",
    "pop_local = pd.DataFrame(local_rows).sort_values(\"year\").reset_index(drop=True)\n",
    "print(\"local population years found:\", pop_local[\"year\"].tolist() if not pop_local.empty else \"NONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd00a3e-667a-49f2-9f0e-e7989afee550",
   "metadata": {},
   "outputs": [],
   "source": [
    "need_years = sorted(set(TARGET_YEARS) - set(pop_local[\"year\"].tolist() if not pop_local.empty else []))\n",
    "api_rows = []\n",
    "for y in need_years:\n",
    "    url = f\"https://api.census.gov/data/{y}/acs/acs5\"\n",
    "    params = {\"get\": \"NAME,B01003_001E\", \"for\": f\"county:{GEOID_DAVIDSON['county']}\", \"in\": f\"state:{GEOID_DAVIDSON['state']}\"}\n",
    "    if os.getenv(\"CENSUS_API_KEY\"): params[\"key\"] = os.getenv(\"CENSUS_API_KEY\")\n",
    "    r = requests.get(url, params=params, timeout=30); r.raise_for_status()\n",
    "    js = r.json()\n",
    "    api_rows.append({\"year\": y, \"population_total\": int(js[1][1])})\n",
    "\n",
    "pop_api = pd.DataFrame(api_rows) if api_rows else pd.DataFrame(columns=[\"year\",\"population_total\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b5d99b-1264-47d6-a54d-adcf42f6c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_clean = (pd.concat([pop_local, pop_api], ignore_index=True)\n",
    "               .drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
    "               .sort_values(\"year\")\n",
    "               .reset_index(drop=True))\n",
    "\n",
    "print(\"final ACS end-years:\", pop_clean[\"year\"].tolist())\n",
    "display(pop_clean.tail(10))\n",
    "\n",
    "# save for reuse\n",
    "OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "(pop_clean.sort_values(\"year\")).to_csv(OUT_DIR / \"pop_acs5_davidson_2014_2023.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee659417-e173-4c6b-83a1-e4e9bc516f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_FIPS  = \"47\"   # Tennessee\n",
    "COUNTY_FIPS = \"037\"  # Davidson\n",
    "YEARS = list(range(2014, 2024))  # ACS 5-year end-years\n",
    "\n",
    "rows = []\n",
    "for y in YEARS:\n",
    "    url = f\"https://api.census.gov/data/{y}/acs/acs5\"\n",
    "    params = {\n",
    "        \"get\": \"NAME,B01003_001E\",  # total population\n",
    "        \"for\": f\"county:{COUNTY_FIPS}\",\n",
    "        \"in\":  f\"state:{STATE_FIPS}\",\n",
    "    }\n",
    "    # optional: use your key if you have one set\n",
    "    if os.getenv(\"CENSUS_API_KEY\"):\n",
    "        params[\"key\"] = os.getenv(\"CENSUS_API_KEY\")\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    rows.append({\"year\": y, \"population_total\": int(js[1][1])})\n",
    "\n",
    "pop_clean = (pd.DataFrame(rows)\n",
    "               .sort_values(\"year\")\n",
    "               .reset_index(drop=True))\n",
    "\n",
    "# persist for reproducibility\n",
    "(OUT_DIR / \"pop_acs5_davidson_2014_2023.csv\").write_text(\n",
    "    pop_clean.to_csv(index=False)\n",
    ")\n",
    "\n",
    "print(\"ACS (county) end-years:\", pop_clean[\"year\"].tolist())\n",
    "print(\"2023 population:\", int(pop_clean.loc[pop_clean[\"year\"]==2023,\"population_total\"].iloc[0]))\n",
    "display(pop_clean.tail(10))\n",
    "\n",
    "# sanity: fail fast if 2023 is not ~700k\n",
    "val_2023 = int(pop_clean.loc[pop_clean[\"year\"]==2023,\"population_total\"].iloc[0])\n",
    "assert 650_000 <= val_2023 <= 800_000, f\"Unexpected 2023 population {val_2023} — check ACS fetch.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f6249-10b3-4337-b9ee-7829d3b880c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population prep — creates pop_clean with ['year','population_total']\n",
    "# Looks for a CSV under data/, data_raw/, or outputs/ that likely contains population by year.\n",
    "\n",
    "SEARCH_DIRS = [Path(\"data\"), Path(\"data_raw\"), Path(\"outputs\")]\n",
    "candidates = []\n",
    "for d in SEARCH_DIRS:\n",
    "    if d.exists():\n",
    "        candidates += [p for p in d.rglob(\"*.csv\") if (\"pop\" in p.name.lower() or \"acs\" in p.name.lower())]\n",
    "\n",
    "if not candidates:\n",
    "    raise FileNotFoundError(\n",
    "        \"No candidate population CSVs found under data/, data_raw/, or outputs/. \"\n",
    "        \"Put a CSV with 'pop' or 'acs' in the filename, or use the manual fallback (see below).\"\n",
    "    )\n",
    "\n",
    "def _pick_year_col(df):\n",
    "    # prefers explicit 'year' column; fallback = a column that looks like years 1990–2035\n",
    "    for c in df.columns:\n",
    "        if \"year\" in str(c).lower():\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        if s.between(1990, 2035).mean() > 0.5:\n",
    "            return c\n",
    "\n",
    "def _pick_pop_col(df):\n",
    "    # prefers a column name containing 'pop'; fallback = numeric column with the largest value range\n",
    "    for c in df.columns:\n",
    "        if \"pop\" in str(c).lower():\n",
    "            return c\n",
    "    best, best_span = None, -1\n",
    "    for c in df.columns:\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        if s.notna().mean() > 0.5:\n",
    "            span = (s.max() - s.min()) if s.notna().any() else -1\n",
    "            if span > best_span:\n",
    "                best, best_span = c, span\n",
    "    return best\n",
    "\n",
    "pop_clean = None\n",
    "used_path = None\n",
    "for pth in candidates:\n",
    "    try:\n",
    "        dfp = pd.read_csv(pth, low_memory=False)\n",
    "    except Exception:\n",
    "        continue\n",
    "    ycol = _pick_year_col(dfp)\n",
    "    pcol = _pick_pop_col(dfp)\n",
    "    if ycol is None or pcol is None:\n",
    "        continue\n",
    "\n",
    "    tmp = dfp[[ycol, pcol]].copy()\n",
    "    tmp.columns = [\"year\", \"population_total\"]\n",
    "    tmp[\"year\"] = pd.to_numeric(tmp[\"year\"], errors=\"coerce\")\n",
    "    tmp[\"population_total\"] = pd.to_numeric(tmp[\"population_total\"], errors=\"coerce\")\n",
    "    tmp = tmp.dropna(subset=[\"year\",\"population_total\"])\n",
    "    if tmp.empty:\n",
    "        continue\n",
    "\n",
    "    pop_clean = (tmp.assign(year=lambda d: d[\"year\"].round().astype(int))\n",
    "                      .query(\"1990 <= year <= 2035\")\n",
    "                      .drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
    "                      .sort_values(\"year\")\n",
    "                      .reset_index(drop=True))\n",
    "    used_path = pth\n",
    "    break\n",
    "\n",
    "if pop_clean is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not auto-detect a population file with clear 'year' and 'population' columns.\\n\"\n",
    "        \"Manual fallback: set CSV_PATH below and build pop_clean from it.\"\n",
    "    )\n",
    "\n",
    "print(\"population source:\", used_path)\n",
    "display(pop_clean.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b291a425-dca4-4b3e-8862-785d97eebf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# builds the ACS 5-year end-year table expected by the merge\n",
    "# NOTE: this expects pop_clean to exist with columns ['year','population_total'] (as prepared earlier in this notebook).\n",
    "if \"pop_clean\" not in globals():\n",
    "    raise NameError(\n",
    "        \"pop_clean is not defined. Run your population prep cell first to create pop_clean \"\n",
    "        \"(with columns ['year','population_total']).\"\n",
    "    )\n",
    "\n",
    "population_year = (\n",
    "    pop_clean.rename(columns={\"population_total\": \"population\"})[[\"year\",\"population\"]]\n",
    "             .assign(year=pd.to_numeric(lambda d: d[\"year\"], errors=\"coerce\"),\n",
    "                     population=pd.to_numeric(lambda d: d[\"population\"], errors=\"coerce\"))\n",
    "             .dropna(subset=[\"year\",\"population\"])\n",
    "             .assign(year=lambda d: d[\"year\"].round().astype(int))\n",
    "             .query(\"1990 <= year <= 2035\")\n",
    "             .drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
    "             .sort_values(\"year\")\n",
    "             .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"ACS end-years:\", population_year[\"year\"].astype(int).tolist())\n",
    "display(population_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3ec93-5958-4445-9b77-5589713b1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- diagnostics: why are the plots a single point? ---\n",
    "# traffic coverage\n",
    "t_years = traffic_year[\"year\"].astype(int).sort_values().tolist()\n",
    "print(\"traffic years:\", t_years[0], \"→\", t_years[-1], \"| count:\", len(t_years))\n",
    "\n",
    "# which end-years actually have a 5y mean?\n",
    "ty_idx = traffic_year.set_index(\"year\").sort_index()\n",
    "t5 = ty_idx[\"aadt_total\"].rolling(window=5, min_periods=5).mean()\n",
    "have_5y = t5[t5.notna()].index.astype(int).tolist()\n",
    "print(\"traffic 5y means available for end-years:\", have_5y)\n",
    "\n",
    "# population ACS end-years\n",
    "p_years = population_year[\"year\"].astype(int).sort_values().tolist()\n",
    "print(\"population ACS end-years:\", p_years)\n",
    "\n",
    "# overlap that survives the merge\n",
    "overlap = sorted(set(have_5y) & set(p_years))\n",
    "print(\"overlap (plotted points):\", overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a653ff-f232-429a-91c6-233c8fd9965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aligns traffic to ACS 5-year end-years and computes per-capita metrics\n",
    "\n",
    "# standardizes traffic and population shapes\n",
    "traffic_year_fix = traffic_year.loc[:, [\"year\",\"aadt_total\"]].copy()\n",
    "population_year_fix = population_year.loc[:, [\"year\",\"population\"]].copy()\n",
    "\n",
    "# builds year-indexed traffic frame and computes strict 5-year rolling mean\n",
    "ty = traffic_year_fix.set_index(\"year\").sort_index()\n",
    "ty[\"aadt_total_5y_mean\"] = ty[\"aadt_total\"].rolling(window=5, min_periods=5).mean()\n",
    "\n",
    "# prepares merge keys (ACS uses end-year label)\n",
    "traffic_5y = (ty.reset_index()\n",
    "                .rename(columns={\"year\": \"end_year\"})\n",
    "                .loc[:, [\"end_year\", \"aadt_total_5y_mean\"]])\n",
    "pop_5y = population_year_fix.rename(columns={\"year\": \"end_year\"})\n",
    "\n",
    "# merges and computes per-capita 5-year metric\n",
    "df5 = (\n",
    "    pop_5y.merge(traffic_5y, on=\"end_year\", how=\"left\")\n",
    "          .assign(\n",
    "              period=lambda d: d[\"end_year\"].apply(lambda y: f\"{y-4}–{y}\"),\n",
    "              aadt_per_capita_5y=lambda d: d[\"aadt_total_5y_mean\"] / d[\"population\"]\n",
    "          )\n",
    "          .dropna(subset=[\"aadt_total_5y_mean\",\"population\"])\n",
    "          .sort_values(\"end_year\")\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"rows after 5-year alignment:\", len(df5))\n",
    "display(df5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c576915-d676-4327-8e5e-492d9d8dc174",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = (traffic_year.set_index(\"year\")[\"aadt_total\"]\n",
    "      .rolling(5, min_periods=5).mean().dropna())\n",
    "overlap = sorted(set(t5.index.astype(int)) & set(population_year[\"year\"].astype(int)))\n",
    "print(\"5y traffic end-years:\", t5.index.astype(int).tolist())\n",
    "print(\"population end-years:\", population_year[\"year\"].astype(int).tolist())\n",
    "print(\"overlap:\", overlap)  # should show multiple: [2017, 2018, …, 2023]\n",
    "print(\"2023 per-capita preview:\",\n",
    "      float(t5.loc[2023]) / float(population_year.set_index(\"year\").loc[2023,\"population\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db89e3-f4ab-4b30-bbe6-5e078e4c12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writes merged table and figures used in the deck\n",
    "\n",
    "# CSV\n",
    "merged_csv = OUT_DIR / \"traffic_population_merged_5y.csv\"\n",
    "df5.to_csv(merged_csv, index=False)\n",
    "print(\"saved:\", merged_csv)\n",
    "\n",
    "# figures\n",
    "# 1) population trend (5y end-years)\n",
    "plt.figure()\n",
    "plt.plot(df5[\"end_year\"], df5[\"population\"], marker=\"o\")\n",
    "plt.title(\"Population (ACS 5-year end-years)\")\n",
    "plt.xlabel(\"End year\")\n",
    "plt.ylabel(\"Population\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"population_trend_5y.png\", dpi=200)\n",
    "\n",
    "# 2) traffic trend (5y mean)\n",
    "plt.figure()\n",
    "plt.plot(df5[\"end_year\"], df5[\"aadt_total_5y_mean\"], marker=\"o\")\n",
    "plt.title(\"Traffic (5-year mean AADT)\")\n",
    "plt.xlabel(\"End year\")\n",
    "plt.ylabel(\"Total AADT (5-year mean)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"traffic_trend_5y.png\", dpi=200)\n",
    "\n",
    "# 3) population vs traffic (5y)\n",
    "plt.figure()\n",
    "plt.scatter(df5[\"population\"], df5[\"aadt_total_5y_mean\"])\n",
    "for _, r in df5.iterrows():\n",
    "    plt.annotate(str(int(r[\"end_year\"])), (r[\"population\"], r[\"aadt_total_5y_mean\"]), xytext=(4,4), textcoords=\"offset points\")\n",
    "plt.title(\"Population vs Traffic (5-year mean)\")\n",
    "plt.xlabel(\"Population (ACS end-year)\")\n",
    "plt.ylabel(\"Total AADT (5-year mean)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"pop_vs_traffic_scatter_5y.png\", dpi=200)\n",
    "\n",
    "# 4) per-capita congestion proxy (5y)\n",
    "plt.figure()\n",
    "plt.plot(df5[\"end_year\"], df5[\"aadt_per_capita_5y\"], marker=\"o\")\n",
    "plt.title(\"Traffic per Capita (5-year mean)\")\n",
    "plt.xlabel(\"End year\")\n",
    "plt.ylabel(\"AADT per person\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"per_capita_congestion_5y.png\", dpi=200)\n",
    "\n",
    "print(\"saved figures to:\", FIG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf16f95a-97be-4e70-9c27-784343963c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumes df5 exists with columns: end_year, population, aadt_total_5y_mean, aadt_per_capita_5y\n",
    "import numpy as np\n",
    "\n",
    "stats = {}\n",
    "stats[\"Population Δ% (2017→2023)\"] = (df5.loc[df5.end_year.eq(2023), \"population\"].iloc[0] /\n",
    "                                      df5.loc[df5.end_year.eq(2017), \"population\"].iloc[0] - 1) * 100\n",
    "stats[\"AADT Δ% (2017→2023)\"] = (df5.loc[df5.end_year.eq(2023), \"aadt_total_5y_mean\"].iloc[0] /\n",
    "                                df5.loc[df5.end_year.eq(2017), \"aadt_total_5y_mean\"].iloc[0] - 1) * 100\n",
    "stats[\"AADT per capita Δ% (2017→2023)\"] = (df5.loc[df5.end_year.eq(2023), \"aadt_per_capita_5y\"].iloc[0] /\n",
    "                                           df5.loc[df5.end_year.eq(2017), \"aadt_per_capita_5y\"].iloc[0] - 1) * 100\n",
    "corr = np.corrcoef(df5[\"population\"], df5[\"aadt_total_5y_mean\"])[0,1]\n",
    "print({k: f\"{v:.2f}%\" for k,v in stats.items()})\n",
    "print(f\"Pop–AADT correlation (5y): {corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6bebc6-9f29-48cd-8c1c-0fb9473c3587",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df5[\"population\"].values\n",
    "y = df5[\"aadt_total_5y_mean\"].values\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y)\n",
    "for _, r in df5.iterrows():\n",
    "    plt.annotate(str(int(r[\"end_year\"])), (r[\"population\"], r[\"aadt_total_5y_mean\"]), xytext=(4,4), textcoords=\"offset points\")\n",
    "xx = np.linspace(x.min(), x.max(), 100)\n",
    "plt.plot(xx, m*xx + b)\n",
    "plt.title(\"Population vs Traffic (5-year mean) with trend line\")\n",
    "plt.xlabel(\"Population (ACS end-year)\")\n",
    "plt.ylabel(\"Total AADT (5-year mean)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"pop_vs_traffic_scatter_5y_trend2.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c642ae-bde6-44ab-b7a1-a465fbc02351",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(df5[\"end_year\"], df5[\"aadt_per_capita_5y\"], marker=\"o\")\n",
    "plt.title(\"Traffic per Capita (5-year mean)\")\n",
    "plt.xlabel(\"End year\"); plt.ylabel(\"AADT per person\")\n",
    "plt.axvspan(2020, 2021, alpha=0.15)  # lightly highlight COVID period\n",
    "plt.grid(True, alpha=0.3); plt.tight_layout()\n",
    "plt.savefig(FIG_DIR/\"per_capita_congestion_5y_covidshade2.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0873d088-88f4-43b4-9a7c-d9ceebdb04a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d66da-e519-4bb8-abb8-1b45cb5cde1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c6e83-9cea-4142-aca2-74d310a55977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6223d1-ce6b-41fc-9f80-80d840255e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e1c376-2804-4042-8364-093d45246fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a96922-029d-4278-9044-1941f37060cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ddccd-0bdb-480c-a7f0-8d350048698a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf734da-fbf0-4e19-b608-30e4acc8de93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6466cd44-9757-4157-80c1-1ceeeff920c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b45d4c5-398d-4e28-b673-501ac5afd398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd5012-f6fc-437f-91d9-3c0ddafd6b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae10a0-9506-4afe-8aa0-732b3c2c5e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed5f355-bee8-4586-b9f1-df99fd7e6964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e547ce1-e539-4505-bf80-039aa33f46e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968cdeb-d960-4cd4-9663-c63a5dfed0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5df34-b1b5-4f78-93e8-be0362b59afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c9500-408c-42b0-8d44-88adbec9d1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9810e-b712-4dfa-8ed4-0e2aadf3c016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de740c0-b32c-41ea-816a-7a27b90b4555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31150199-c567-4dee-a53e-7e2171e97c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb16ec1e-37d2-4c39-b6bc-96693f844ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92188cd5-4b53-48b4-a1fb-99956aaefa8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375542f-c0b1-4aeb-b697-e8c36f68071a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07242b4e-934c-4a03-a6bc-bef3189372d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916da89-aef0-4cf9-9617-e04e888a7515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70b1a4-0f1a-43ef-9b3b-d846a0a14516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c92fc-f7e7-4a71-8dc7-96e3c6c02296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c82595-9821-43f8-87ec-fc5ced09ce97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3559c2a-e68e-4083-ac1d-cfd7abcf2f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea986f-583c-4f96-99a2-d2ee1abe8bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef322a4-337f-402e-ba2c-12cbbcf56cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223b3e6-431a-4748-804b-e6a59b1f5acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc107b-0138-4675-ad6f-c0a033048fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c272fbab-561e-49b9-a588-fc1ad74d0149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
