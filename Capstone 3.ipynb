{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5f035-58da-410a-aa72-e4f0908eeb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63c7407-baf6-4f6d-a649-4eff544b19d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path(\".\")  # folder where your CSVs live\n",
    "OUT  = Path(\"./acs_clean\"); OUT.mkdir(exist_ok=True)\n",
    "\n",
    "FILES = {\n",
    "    \"population\": DATA/\"USCPOPDATA.csv\",\n",
    "    \"housing\": DATA/\"USCHOUSINGDATA.csv\",\n",
    "    \"tenure\": DATA/\"USCHOUSETENUREDATA.csv\",\n",
    "    \"commute_time\": DATA/\"USCCOMMUTETIMEDATA.csv\",\n",
    "    \"transport_type\": DATA/\"USCTRANSPORTTYPEDATA.csv\",\n",
    "    \"vehicles\": DATA/\"USCVEHICLESAVAILABLEDATA.csv\",\n",
    "    \"sex_by_age\": DATA/\"USCSEXBYAGEDATA.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9387f4b-f72e-4026-acdf-d787c7c0326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _first_estimate_column(df):\n",
    "    # Find the Estimate column like \"Davidson County, Tennessee!!Estimate\"\n",
    "    for c in df.columns:\n",
    "        if isinstance(c, str) and c.endswith(\"!!Estimate\"):\n",
    "            return c\n",
    "    raise ValueError(\"Estimate column not found (expected to end with '!!Estimate').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58e072-2dc7-4b3c-8c19-18698b0c2d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _first_moe_column(df):\n",
    "    # Find the Margin of Error column like \"Davidson County, Tennessee!!Margin of Error\"\n",
    "    for c in df.columns:\n",
    "        if isinstance(c, str) and c.endswith(\"!!Margin of Error\"):\n",
    "            return c\n",
    "    return None  # some downloads may lack MOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5878a925-3916-4c3c-9c25-e427a55a3a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_number(x):\n",
    "    # Strip commas, ±, spaces; coerce to numeric\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    return pd.to_numeric(str(x).replace(\",\", \"\").replace(\"±\",\"\").strip(), errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae69cda-051f-45f3-9076-22df3b97cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_acs_single_table(path, table_name):\n",
    "    df = pd.read_csv(path)\n",
    "    cat_col = \"Label (Grouping)\" if \"Label (Grouping)\" in df.columns else \"Label\"\n",
    "    est_col = _first_estimate_column(df)\n",
    "    moe_col = _first_moe_column(df)\n",
    "\n",
    "    keep = [cat_col, est_col] + ([moe_col] if moe_col else [])\n",
    "    out = df[keep].copy()\n",
    "\n",
    "    out = out.rename(columns={\n",
    "        cat_col: \"category\",\n",
    "        est_col: \"estimate\",\n",
    "        (moe_col or \"moe\"): \"moe\"\n",
    "    })\n",
    "\n",
    "    out[\"estimate\"] = out[\"estimate\"].apply(_to_number)\n",
    "    if moe_col:\n",
    "        out[\"moe\"] = out[\"moe\"].apply(_to_number)\n",
    "    else:\n",
    "        out[\"moe\"] = np.nan\n",
    "\n",
    "    # trim whitespace and normalize the category text\n",
    "    out[\"category\"] = out[\"category\"].astype(str).str.replace(r\"\\u00a0\", \" \", regex=True).str.strip()\n",
    "    out.insert(0, \"table\", table_name)       # add table name\n",
    "    out.insert(1, \"geo\", \"Davidson County, TN\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3016b24-fba9-4dac-8ba1-861984735bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = []\n",
    "for name, path in FILES.items():\n",
    "    cleaned.append(clean_acs_single_table(path, name))\n",
    "acs_long = pd.concat(cleaned, ignore_index=True)\n",
    "\n",
    "acs_long.to_csv(OUT/\"acs_davidson_2023_long.csv\", index=False)\n",
    "acs_long.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1486cb-2be5-4209-bc88-2d3429fd2039",
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_long.groupby(\"table\").size().rename(\"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc4d5b-a491-4fff-a7eb-ac4fe178f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tbl in FILES.keys():\n",
    "    print(\"\\n===\", tbl, \"===\")\n",
    "    display(acs_long.loc[acs_long.table.eq(tbl), [\"category\",\"estimate\",\"moe\"]].head(8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106aa34-e1b8-4153-9dab-797e51d9dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm_cat(s: str) -> str:\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).replace(\"\\u00a0\",\" \")\n",
    "    s = re.sub(r\"\\s+\",\" \", s).strip().rstrip(\":\")\n",
    "    return s.lower()\n",
    "\n",
    "def _pick_any(df: pd.DataFrame, patterns):\n",
    "    \"\"\"Return the first matching estimate for any pattern (exact OR prefix if pattern ends with ':').\"\"\"\n",
    "    if isinstance(patterns, str):\n",
    "        patterns = [patterns]\n",
    "    if df.empty:\n",
    "        return None\n",
    "    cats = df[\"category\"].map(_norm_cat)\n",
    "    for pat in patterns:\n",
    "        pat_n = _norm_cat(pat)\n",
    "        allow_prefix = pat.strip().endswith(\":\")\n",
    "        mask = cats.str.startswith(pat_n) if allow_prefix else (cats == pat_n)\n",
    "        vals = pd.to_numeric(df.loc[mask, \"estimate\"], errors=\"coerce\")\n",
    "        if not vals.empty:\n",
    "            v = vals.max()\n",
    "            if pd.notna(v):\n",
    "                return float(v)\n",
    "    return None\n",
    "\n",
    "def summarize_tables(acs_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = {}\n",
    "\n",
    "    # Population\n",
    "    pop = acs_long.query(\"table == 'population'\")\n",
    "    out[\"population_total\"] = _pick_any(pop, [\"total:\", \"total population\"])\n",
    "\n",
    "    # Housing units\n",
    "    housing = acs_long.query(\"table == 'housing'\")\n",
    "    out[\"housing_units_total\"] = _pick_any(housing, [\"total housing units\", \"total:\"])\n",
    "\n",
    "    # Tenure\n",
    "    tenure = acs_long.query(\"table == 'tenure'\")\n",
    "    out[\"occupied_total\"]  = _pick_any(tenure, [\"total:\"])\n",
    "    out[\"owner_occupied\"]  = _pick_any(tenure, [\"owner occupied\", \"occupied housing units:\"])\n",
    "    out[\"renter_occupied\"] = _pick_any(tenure, [\"renter occupied\", \"occupied housing units: renter occupied\"])\n",
    "\n",
    "    # Vehicles available\n",
    "    veh = acs_long.query(\"table == 'vehicles'\")\n",
    "    out[\"hh_total_for_veh\"] = _pick_any(veh, [\"total:\"])\n",
    "    out[\"hh_0_veh\"]         = _pick_any(veh, [\"no vehicle available\", \"no vehicles available\"])\n",
    "    out[\"hh_1_veh\"]         = _pick_any(veh, [\"1 vehicle available\"])\n",
    "    out[\"hh_2_veh\"]         = _pick_any(veh, [\"2 vehicles available\"])\n",
    "    out[\"hh_3plus_veh\"]     = _pick_any(veh, [\"3 or more vehicles available\", \"3 vehicles available\"])\n",
    "\n",
    "    # Commute time distribution\n",
    "    ct = acs_long.query(\"table == 'commute_time'\")\n",
    "    out[\"workers_total\"]    = _pick_any(ct, [\"total:\"])\n",
    "    out[\"gt_30min_commute\"] = sum([\n",
    "        _pick_any(ct, \"30 to 34 minutes\") or 0,\n",
    "        _pick_any(ct, \"35 to 39 minutes\") or 0,\n",
    "        _pick_any(ct, \"40 to 44 minutes\") or 0,\n",
    "        _pick_any(ct, \"45 to 59 minutes\") or 0,\n",
    "        _pick_any(ct, \"60 to 89 minutes\") or 0,\n",
    "        _pick_any(ct, \"90 or more minutes\") or 0,\n",
    "    ])\n",
    "\n",
    "    # Transport type / mode share\n",
    "    mode = acs_long.query(\"table == 'transport_type'\")\n",
    "    out[\"mode_total\"]         = _pick_any(mode, [\"total:\"])\n",
    "    out[\"drive_alone\"]        = _pick_any(mode, [\"drove alone\"])  \n",
    "    out[\"carpool\"]            = _pick_any(mode, [\"carpooled:\"]) \n",
    "    out[\"public_transport\"]   = _pick_any(mode, [\"public transportation (excluding taxicab)\"])\n",
    "    out[\"walked\"]             = _pick_any(mode, [\"walked\"])\n",
    "    out[\"bicycle\"]            = _pick_any(mode, [\"bicycle\"])\n",
    "    out[\"worked_from_home\"]   = _pick_any(mode, [\"worked from home\"])\n",
    "\n",
    "    return pd.DataFrame([out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0e3b77-3ca8-467c-a76e-7eacffb6fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_wide = summarize_tables(acs_long)\n",
    "print(type(acs_wide), acs_wide.shape)\n",
    "display(acs_wide.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89618b3e-0c71-48fd-b335-a6b9efc9f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = acs_wide.iloc[0]\n",
    "\n",
    "acs_metrics = pd.Series({\n",
    "    # mode shares\n",
    "    \"drive_alone_share\":        w[\"drive_alone\"] / w[\"mode_total\"],\n",
    "    \"carpool_share\":            w[\"carpool\"] / w[\"mode_total\"],\n",
    "    \"public_transport_share\":   w[\"public_transport\"] / w[\"mode_total\"],\n",
    "    \"walk_share\":               w[\"walked\"] / w[\"mode_total\"],\n",
    "    \"bike_share\":               w[\"bicycle\"] / w[\"mode_total\"],\n",
    "    \"wfh_share\":                w[\"worked_from_home\"] / w[\"mode_total\"],\n",
    "\n",
    "    # vehicles per household shares\n",
    "    \"zero_vehicle_share\":       w[\"hh_0_veh\"] / w[\"hh_total_for_veh\"],\n",
    "    \"one_vehicle_share\":        w[\"hh_1_veh\"] / w[\"hh_total_for_veh\"],\n",
    "    \"two_vehicle_share\":        w[\"hh_2_veh\"] / w[\"hh_total_for_veh\"],\n",
    "    \"three_plus_vehicle_share\": w[\"hh_3plus_veh\"] / w[\"hh_total_for_veh\"],\n",
    "\n",
    "    # tenure shares\n",
    "    \"owner_share\":              w[\"owner_occupied\"] / w[\"occupied_total\"],\n",
    "    \"renter_share\":             w[\"renter_occupied\"] / w[\"occupied_total\"],\n",
    "})\n",
    "\n",
    "acs_metrics.to_frame(\"value\").to_csv(OUT/\"acs_davidson_2023_metrics.csv\")\n",
    "acs_metrics.to_frame(\"value\").round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34bc11c-a99a-4604-be34-7dc770c9e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mode shares sum:\", float(acs_metrics[[\n",
    "    \"drive_alone_share\",\"carpool_share\",\"public_transport_share\",\"walk_share\",\"bike_share\",\"wfh_share\"\n",
    "]].sum()))\n",
    "\n",
    "print(\"vehicle shares sum:\", float(acs_metrics[[\n",
    "    \"zero_vehicle_share\",\"one_vehicle_share\",\"two_vehicle_share\",\"three_plus_vehicle_share\"\n",
    "]].sum()))\n",
    "\n",
    "print(\"tenure shares sum:\", float(acs_metrics[[\"owner_share\",\"renter_share\"]].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c09ca83-27b9-4972-9a8e-a45b0cacde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = acs_metrics[[\n",
    "    \"drive_alone_share\",\"carpool_share\",\"public_transport_share\",\n",
    "    \"walk_share\",\"bike_share\",\"wfh_share\"\n",
    "]].sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(ms.index.str.replace(\"_share\",\"\").str.replace(\"_\",\" \").str.title(), ms.values)\n",
    "plt.title(\"Mode Shares — Davidson County (ACS 5-Year 2019–2023)\")\n",
    "plt.ylabel(\"Share of commuters\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db7210-9ba8-496b-a820-2d67137b5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_all = pd.concat([acs_wide.T.rename(columns={0:\"value\"}), acs_metrics.to_frame(\"value\")])\n",
    "out_all.to_csv(OUT/\"acs_davidson_2023_summary_plus_metrics.csv\")\n",
    "out_all\n",
    "print(out_all.reset_index().rename(columns={\"index\":\"metric\"}).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ae6834-9921-4ad4-b8b5-e68ee991bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = (acs_long.loc[acs_long.table.eq(\"transport_type\"), \"category\"]\n",
    "        .dropna().drop_duplicates().sort_values())\n",
    "cats.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b837b7-a4df-4980-94fb-b4191364b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(acs_long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd8efad-864d-4f53-8f4a-568d7bb681cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(acs_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03f910-77f7-4945-b3c4-714abd525618",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tbl in [\"population\",\"housing\",\"tenure\",\"vehicles\",\"commute_time\",\"transport_type\"]:\n",
    "    print(\"\\n=== \", tbl, \" ===\")\n",
    "    cats = (acs_long\n",
    "            .loc[acs_long.table.eq(tbl), \"category\"]\n",
    "            .dropna()\n",
    "            .drop_duplicates()\n",
    "            .sort_values())\n",
    "    display(cats.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd8ae7-58b8-45af-938c-fce86859978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = acs_wide.iloc[0]\n",
    "\n",
    "acs_metrics = pd.Series({\n",
    "    \"owner_share\": w[\"owner_occupied\"] / w[\"occupied_total\"] if w[\"occupied_total\"] else np.nan,\n",
    "    \"renter_share\": w[\"renter_occupied\"] / w[\"occupied_total\"] if w[\"occupied_total\"] else np.nan,\n",
    "    \"zero_vehicle_share\": w[\"hh_0_veh\"] / w[\"hh_total_for_veh\"] if w[\"hh_total_for_veh\"] else np.nan,\n",
    "    \"one_vehicle_share\":  w[\"hh_1_veh\"] / w[\"hh_total_for_veh\"] if w[\"hh_total_for_veh\"] else np.nan,\n",
    "    \"two_vehicle_share\":  w[\"hh_2_veh\"] / w[\"hh_total_for_veh\"] if w[\"hh_total_for_veh\"] else np.nan,\n",
    "    \"three_plus_vehicle_share\": w[\"hh_3plus_veh\"] / w[\"hh_total_for_veh\"] if w[\"hh_total_for_veh\"] else np.nan,\n",
    "    \"gt_30min_commute_share\": w[\"gt_30min_commute\"] / w[\"workers_total\"] if w[\"workers_total\"] else np.nan,\n",
    "    \"drive_alone_share\":  w[\"drive_alone\"] / w[\"mode_total\"] if w[\"mode_total\"] else np.nan,\n",
    "    \"carpool_share\":      w[\"carpool\"] / w[\"mode_total\"] if w[\"mode_total\"] else np.nan,\n",
    "    \"public_transport_share\": w[\"public_transport\"] / w[\"mode_total\"] if w[\"mode_total\"] else np.nan,\n",
    "    \"walk_share\":         w[\"walked\"] / w[\"mode_total\"] if w[\"mode_total\"] else np.nan,\n",
    "    \"bike_share\":         w[\"bicycle\"] / w[\"mode_total\"] if w[\"mode_total\"] else np.nan,\n",
    "    \"wfh_share\":          w[\"worked_from_home\"] / w[\"mode_total\"] if w[\"mode_total\"] else np.nan,\n",
    "})\n",
    "\n",
    "acs_metrics.to_frame(\"value\").to_csv(OUT/\"acs_davidson_2023_metrics.csv\")\n",
    "acs_metrics.to_frame(\"value\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d58861-1414-4655-a620-94951e14254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tbl in FILES.keys():\n",
    "    acs_long.loc[acs_long.table.eq(tbl), :].to_csv(OUT/f\"acs_{tbl}_2023_long.csv\", index=False)\n",
    "print(\"Wrote individual cleaned tables to:\", OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb69a3-079e-4ffa-adde-1d7817221172",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATES = {\n",
    "    \"income\":       [\"USCINCOMEDATA.csv\", \"USCMEDHHINCOME.csv\"],\n",
    "    \"poverty\":      [\"USCPOVERTYDATA.csv\"],\n",
    "    \"employment\":   [\"USCEMPLOYMENTDATA.csv\"],\n",
    "    \"education\":    [\"USCEDUCATIONDATA.csv\"],\n",
    "    \"rent\":         [\"USCMEDRENT.csv\", \"USCMEDRENTDATA.csv\", \"USCRENTPAYDATA.csv\"],\n",
    "    \"home_value\":   [\"USCHOMEVALUEDATA.csv\", \"USCMEDHOMEVAL.csv\"],\n",
    "}\n",
    "\n",
    "def find_file(names, search_dir=Path(\".\")):\n",
    "    for name in names:\n",
    "        p = search_dir / name\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "NEW_FILES = {k: find_file(v) for k, v in CANDIDATES.items()}\n",
    "FOUND = {k: v for k, v in NEW_FILES.items() if v is not None}\n",
    "MISSING = [k for k, v in NEW_FILES.items() if v is None]\n",
    "\n",
    "print(\"Found:\", FOUND)\n",
    "print(\"Missing:\", MISSING or \"None\")\n",
    "\n",
    "# ❷ Merge into your existing FILES dict only for ones we actually found\n",
    "try:\n",
    "    FILES.update(FOUND)\n",
    "except NameError:\n",
    "    FILES = FOUND.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb88dad-af1b-439a-97f1-36149888c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_long = []\n",
    "for name in [\"income\",\"poverty\",\"employment\",\"education\",\"rent\",\"home_value\"]:\n",
    "    if name in FILES:\n",
    "        new_long.append(clean_acs_single_table(FILES[name], name))\n",
    "\n",
    "if new_long:\n",
    "    new_long = pd.concat(new_long, ignore_index=True)\n",
    "    try:\n",
    "        acs_long = pd.concat([acs_long, new_long], ignore_index=True)\n",
    "    except NameError:\n",
    "        acs_long = new_long.copy()\n",
    "else:\n",
    "    print(\"No new ACS tables were found on disk to append.\")\n",
    "\n",
    "# optional: save the refreshed long file alongside your existing outputs\n",
    "OUT.mkdir(exist_ok=True)\n",
    "(OUT / \"acs_davidson_long_latest.csv\").write_text(acs_long.to_csv(index=False))\n",
    "print(\"acs_long rows:\", len(acs_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9bd832-47b8-478f-bdcc-fb2eeecc157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_tables_extended(acs_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = {}\n",
    "\n",
    "    # --- already in your workbook ---\n",
    "    pop   = acs_long.query(\"table == 'population'\")\n",
    "    hous  = acs_long.query(\"table == 'housing'\")\n",
    "    ten   = acs_long.query(\"table == 'tenure'\")\n",
    "    veh   = acs_long.query(\"table == 'vehicles'\")\n",
    "    ct    = acs_long.query(\"table == 'commute_time'\")\n",
    "    mode  = acs_long.query(\"table == 'transport_type'\")\n",
    "\n",
    "    out[\"population_total\"]      = _pick_any(pop, [\"total:\", \"total population\"])\n",
    "    out[\"housing_units_total\"]   = _pick_any(hous, [\"total housing units\", \"total:\"])\n",
    "    out[\"occupied_total\"]        = _pick_any(ten,  [\"total:\"])\n",
    "    out[\"owner_occupied\"]        = _pick_any(ten,  [\"owner occupied\", \"occupied housing units:\"])\n",
    "    out[\"renter_occupied\"]       = _pick_any(ten,  [\"renter occupied\", \"occupied housing units: renter occupied\"])\n",
    "    out[\"hh_total_for_veh\"]      = _pick_any(veh,  [\"total:\"])\n",
    "    out[\"hh_0_veh\"]              = _pick_any(veh,  [\"no vehicle available\", \"no vehicles available\"])\n",
    "    out[\"hh_1_veh\"]              = _pick_any(veh,  [\"1 vehicle available\"])\n",
    "    out[\"hh_2_veh\"]              = _pick_any(veh,  [\"2 vehicles available\"])\n",
    "    out[\"hh_3plus_veh\"]          = _pick_any(veh,  [\"3 or more vehicles available\", \"3 vehicles available\"])\n",
    "    out[\"workers_total\"]         = _pick_any(ct,   [\"total:\"])\n",
    "    out[\"gt_30min_commute\"]      = sum([\n",
    "        _pick_any(ct, \"30 to 34 minutes\") or 0,\n",
    "        _pick_any(ct, \"35 to 39 minutes\") or 0,\n",
    "        _pick_any(ct, \"40 to 44 minutes\") or 0,\n",
    "        _pick_any(ct, \"45 to 59 minutes\") or 0,\n",
    "        _pick_any(ct, \"60 to 89 minutes\") or 0,\n",
    "        _pick_any(ct, \"90 or more minutes\") or 0,\n",
    "    ])\n",
    "    out[\"mode_total\"]            = _pick_any(mode, [\"total:\"])\n",
    "    out[\"drive_alone\"]           = _pick_any(mode, [\"drove alone\"])\n",
    "    out[\"carpool\"]               = _pick_any(mode, [\"carpooled:\"])  # header with colon matches your labels\n",
    "    out[\"public_transport\"]      = _pick_any(mode, [\"public transportation (excluding taxicab):\"])\n",
    "    out[\"walked\"]                = _pick_any(mode, [\"walked\"])\n",
    "    out[\"bicycle\"]               = _pick_any(mode, [\"bicycle\"])\n",
    "    out[\"worked_from_home\"]      = _pick_any(mode, [\"worked from home\"])\n",
    "\n",
    "    # --- NEW topics (only if present) ---\n",
    "    # Income (B19013)\n",
    "    inc = acs_long.query(\"table == 'income'\")\n",
    "    if not inc.empty:\n",
    "        out[\"median_household_income\"] = _pick_any(inc, [\n",
    "            \"median household income in the past 12 months (in 2023 inflation-adjusted dollars)\",\n",
    "            \"median household income in the past 12 months (in inflation-adjusted dollars)\",\n",
    "            \"median household income in the past 12 months (in 20\",  # fallback prefix\n",
    "        ])\n",
    "\n",
    "    # Poverty (B17001)\n",
    "    pov = acs_long.query(\"table == 'poverty'\")\n",
    "    if not pov.empty:\n",
    "        out[\"poverty_denominator\"] = _pick_any(pov, [\"total:\"])\n",
    "        out[\"below_poverty_total\"] = _pick_any(pov, [\"below poverty level:\"])\n",
    "\n",
    "    # Employment (B23025)\n",
    "    emp = acs_long.query(\"table == 'employment'\")\n",
    "    if not emp.empty:\n",
    "        out[\"pop_16plus\"]       = _pick_any(emp, [\"civilian noninstitutional population\"])\n",
    "        out[\"in_labor_force\"]   = _pick_any(emp, [\"in labor force:\"])\n",
    "        out[\"civilian_lf\"]      = _pick_any(emp, [\"civilian labor force:\"])\n",
    "        out[\"employed\"]         = _pick_any(emp, [\"employed\"])\n",
    "        out[\"unemployed\"]       = _pick_any(emp, [\"unemployed\"])\n",
    "        out[\"not_in_labor\"]     = _pick_any(emp, [\"not in labor force\"])\n",
    "\n",
    "    # Education (B15003, 25+)\n",
    "    edu = acs_long.query(\"table == 'education'\")\n",
    "    if not edu.empty:\n",
    "        out[\"edu_total_25plus\"] = _pick_any(edu, [\"total:\"])\n",
    "        hs_plus = sum([\n",
    "            _pick_any(edu, \"regular high school diploma\") or 0,\n",
    "            _pick_any(edu, \"ged or alternative credential\") or 0,\n",
    "            _pick_any(edu, \"some college, less than 1 year\") or 0,\n",
    "            _pick_any(edu, \"some college, 1 or more years, no degree\") or 0,\n",
    "            _pick_any(edu, \"associate's degree\") or 0,\n",
    "            _pick_any(edu, \"bachelor's degree\") or 0,\n",
    "            _pick_any(edu, \"master's degree\") or 0,\n",
    "            _pick_any(edu, \"professional school degree\") or 0,\n",
    "            _pick_any(edu, \"doctorate degree\") or 0,\n",
    "        ])\n",
    "        bach_plus = sum([\n",
    "            _pick_any(edu, \"bachelor's degree\") or 0,\n",
    "            _pick_any(edu, \"master's degree\") or 0,\n",
    "            _pick_any(edu, \"professional school degree\") or 0,\n",
    "            _pick_any(edu, \"doctorate degree\") or 0,\n",
    "        ])\n",
    "        grad_plus = sum([\n",
    "            _pick_any(edu, \"master's degree\") or 0,\n",
    "            _pick_any(edu, \"professional school degree\") or 0,\n",
    "            _pick_any(edu, \"doctorate degree\") or 0,\n",
    "        ])\n",
    "        out[\"edu_hs_or_higher\"]     = hs_plus\n",
    "        out[\"edu_bachelors_plus\"]   = bach_plus\n",
    "        out[\"edu_grad_prof_plus\"]   = grad_plus\n",
    "\n",
    "    # Housing costs\n",
    "    rent = acs_long.query(\"table == 'rent'\")\n",
    "    if not rent.empty:\n",
    "        out[\"median_gross_rent\"] = _pick_any(rent, [\"median gross rent (dollars)\", \"median gross rent\"])\n",
    "    hv = acs_long.query(\"table == 'home_value'\")\n",
    "    if not hv.empty:\n",
    "        out[\"median_home_value\"] = _pick_any(hv, [\"median value (dollars)\", \"median value\"])\n",
    "\n",
    "    return pd.DataFrame([out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a503bc-b05a-4a80-b2bf-817745e48d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_wide_ext = summarize_tables_extended(acs_long)\n",
    "display(acs_wide_ext.T)\n",
    "\n",
    "w = acs_wide_ext.iloc[0]\n",
    "derived = pd.Series({\n",
    "    \"unemployment_rate\":   (w[\"unemployed\"] / w[\"civilian_lf\"]) if w.get(\"civilian_lf\") else None,\n",
    "    \"labor_participation\": (w[\"in_labor_force\"] / w[\"pop_16plus\"]) if w.get(\"pop_16plus\") else None,\n",
    "    \"poverty_rate\":        (w[\"below_poverty_total\"] / w[\"poverty_denominator\"]) if w.get(\"poverty_denominator\") else None,\n",
    "    \"hs_or_higher_share\":  (w[\"edu_hs_or_higher\"] / w[\"edu_total_25plus\"]) if w.get(\"edu_total_25plus\") else None,\n",
    "    \"bachelors_plus_share\":(w[\"edu_bachelors_plus\"] / w[\"edu_total_25plus\"]) if w.get(\"edu_total_25plus\") else None,\n",
    "    \"grad_prof_plus_share\":(w[\"edu_grad_prof_plus\"] / w[\"edu_total_25plus\"]) if w.get(\"edu_total_25plus\") else None,\n",
    "})\n",
    "\n",
    "out_all_ext = pd.concat([acs_wide_ext.T.rename(columns={0:\"value\"}), derived.to_frame(\"value\")])\n",
    "out_path = OUT / \"acs_davidson_summary_plus_metrics_extended.csv\"\n",
    "out_all_ext.to_csv(out_path)\n",
    "print(\"Wrote:\", out_path)\n",
    "print(out_all_ext.reset_index().rename(columns={\"index\":\"metric\"}).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452e5ff-093d-49e9-93c9-a372c73c4647",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tbl in [\"income\",\"poverty\",\"employment\",\"education\",\"rent\",\"home_value\"]:\n",
    "    if tbl in FILES:\n",
    "        print(\"\\n==\", tbl.upper(), \"==\", FILES[tbl])\n",
    "        cats = (acs_long.loc[acs_long.table.eq(tbl), \"category\"]\n",
    "                .dropna().drop_duplicates().sort_values())\n",
    "        for c in cats[:200]:\n",
    "            print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b66a13-b891-4921-b3e7-825a54a58a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [\"income\",\"poverty\",\"employment\",\"education\",\"rent\",\"home_value\"]:\n",
    "    print(f\"{k:12s} →\", FILES.get(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b4937-4b93-4c4f-9c82-046048ddfba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACS_2023 = {\n",
    "    \"population\":      Path(\"USCPOPDATA.csv\"),\n",
    "    \"housing\":         Path(\"USCHOUSINGDATA.csv\"),\n",
    "    \"tenure\":          Path(\"USCHOUSETENUREDATA.csv\"),\n",
    "    \"vehicles\":        Path(\"USCVEHICLESAVAILABLEDATA.csv\"),\n",
    "    \"commute_time\":    Path(\"USCCOMMUTETIMEDATA.csv\"),\n",
    "    \"transport_type\":  Path(\"USCTRANSPORTTYPEDATA.csv\"),\n",
    "    \"sex_by_age\":      Path(\"USCSEXBYAGEDATA.csv\"),\n",
    "    # add new topics here when you have them (same base name pattern):\n",
    "    # \"income\":        Path(\"USCINCOMEDATA.csv\"),\n",
    "    # \"poverty\":       Path(\"USCPOVERTYDATA.csv\"),\n",
    "    # \"employment\":    Path(\"USCEMPLOYMENTDATA.csv\"),\n",
    "    # \"education\":     Path(\"USCEDUCATIONDATA.csv\"),\n",
    "    # \"rent\":          Path(\"USCRENTPAYDATA.csv\"),\n",
    "    # \"home_value\":    Path(\"USCHOMEVALUEDATA.csv\"),\n",
    "}\n",
    "\n",
    "def with_year(p: Path, year: int) -> Path:\n",
    "    \"\"\"Insert the year just before the .csv extension: name.csv -> nameYYYY.csv\"\"\"\n",
    "    if p.suffix.lower() != \".csv\":\n",
    "        return p  # safety: only handle csvs\n",
    "    return p.with_name(p.stem + str(year) + p.suffix)\n",
    "\n",
    "YEARS = [2023, 2018, 2013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110d900-8a3b-4806-894e-6e758dd3ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_by_year = {\n",
    "    topic: {y: (with_year(base, y) if y != 2023 else base) for y in YEARS}\n",
    "    for topic, base in ACS_2023.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f342b-c014-41ff-afc4-d485deb2854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for topic, by_year in files_by_year.items():\n",
    "    for y, path in by_year.items():\n",
    "        rows.append({\"topic\": topic, \"year\": y, \"path\": str(path), \"exists\": path.exists()})\n",
    "availability = pd.DataFrame(rows).sort_values([\"topic\",\"year\"])\n",
    "print(availability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dadabe6-b9df-4ecb-b965-944622d747e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_topic_year(topic: str, year: int, n: int = 5):\n",
    "    \"\"\"Load and show the top rows of a topic/year file.\"\"\"\n",
    "    path = files_by_year[topic][year]\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"\\n--- {topic.upper()} {year} ---\")\n",
    "    print(df.shape)\n",
    "    display(df.head(n))\n",
    "    return df\n",
    "\n",
    "# Example: preview transport_type for 2018\n",
    "df_transport_2018 = preview_topic_year(\"transport_type\", 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9142e-c92a-4ea7-916d-bb2022e125de",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_topic_year(\"vehicles\", 2013)\n",
    "preview_topic_year(\"housing\", 2023)\n",
    "preview_topic_year(\"sex_by_age\", 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab01229b-e5c4-4903-81e7-7a936812ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expected columns for each topic\n",
    "standard_columns = {\n",
    "    \"population\": [\"population_total\"],\n",
    "    \"housing\": [\"housing_units_total\", \"occupied_total\"],\n",
    "    \"tenure\": [\"owner_occupied\", \"renter_occupied\"],\n",
    "    \"vehicles\": [\"hh_total_for_veh\", \"hh_0_veh\", \"hh_1_veh\", \"hh_2_veh\", \"hh_3plus_veh\"],\n",
    "    \"commute_time\": [\"workers_total\", \"gt_30min_commute\"],\n",
    "    \"transport_type\": [\"mode_total\", \"drive_alone\", \"carpool\", \"public_transport\", \"walked\", \"bicycle\", \"worked_from_home\"],\n",
    "    \"sex_by_age\": [\"male_total\", \"female_total\", \"median_age\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e226b-5fef-4354-b87d-621a1964b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_standardize(topic: str, year: int) -> pd.DataFrame:\n",
    "    \"\"\"Load, clean, and standardize ACS data for a given topic/year.\"\"\"\n",
    "    path = files_by_year[topic][year]\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Select + rename\n",
    "    keep_cols = standard_columns[topic]\n",
    "    df = df.reindex(columns=keep_cols)\n",
    "\n",
    "    # Add topic/year context\n",
    "    df[\"topic\"] = topic\n",
    "    df[\"year\"] = year\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da917b7b-fb85-4bd1-8bff-b4f765ed69f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_topic(topic: str, years=[2013, 2018, 2023]):\n",
    "    frames = [clean_standardize(topic, y) for y in years]\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Example: Combine vehicle availability across years\n",
    "vehicles_all = combine_topic(\"vehicles\")\n",
    "display(vehicles_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808027b-1d75-41dc-802e-052de457bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = {\n",
    "    topic: combine_topic(topic)\n",
    "    for topic in standard_columns.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bdce04-418e-461a-b806-dd2139953136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_columns(topic: str, years=[2013, 2018, 2023]):\n",
    "    expected = set(standard_columns[topic])\n",
    "    \n",
    "    for year in years:\n",
    "        path = files_by_year[topic][year]\n",
    "        df = pd.read_csv(path)\n",
    "        actual = set(df.columns)\n",
    "        \n",
    "        missing = expected - actual\n",
    "        extra   = actual - expected\n",
    "        \n",
    "        print(f\"\\n{topic.upper()} — {year}\")\n",
    "        print(f\"  Missing: {missing if missing else 'None'}\")\n",
    "        print(f\"  Extra:   {extra if extra else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab301077-73bb-4708-8e80-5c9eb4a54184",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in standard_columns.keys():\n",
    "    check_columns(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a0c4d-5377-429f-98fb-3c0d0710bc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_est_col(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Return the column name ending with '!!Estimate' (as in ACS downloads).\"\"\"\n",
    "    for c in df.columns:\n",
    "        if isinstance(c, str) and c.endswith(\"!!Estimate\"):\n",
    "            return c\n",
    "    raise ValueError(\"No '*!!Estimate' column found.\")\n",
    "\n",
    "def _norm_label(s: str) -> str:\n",
    "    \"\"\"Normalize category labels to improve matching.\"\"\"\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).replace(\"\\u00a0\", \" \")              # nbsp\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()             # collapse spaces\n",
    "    s = s.rstrip(\":\")                              # drop trailing colon\n",
    "    return s.lower()\n",
    "\n",
    "def _pick_any(df_cat: pd.DataFrame, patterns) -> float | None:\n",
    "    \"\"\"Pick estimate by matching normalized label against any pattern.\n",
    "       If a pattern ends with ':' we allow prefix matching (hierarchy headers).\"\"\"\n",
    "    if isinstance(patterns, str):\n",
    "        patterns = [patterns]\n",
    "    if df_cat.empty:\n",
    "        return None\n",
    "    labels_n = df_cat[\"category_n\"]\n",
    "    for pat in patterns:\n",
    "        pat_n = _norm_label(pat)\n",
    "        allow_prefix = pat.strip().endswith(\":\")\n",
    "        mask = labels_n.str.startswith(pat_n) if allow_prefix else (labels_n == pat_n)\n",
    "        vals = pd.to_numeric(df_cat.loc[mask, \"estimate\"], errors=\"coerce\")\n",
    "        if not vals.empty:\n",
    "            v = vals.max()\n",
    "            if pd.notna(v):\n",
    "                return float(v)\n",
    "    return None\n",
    "\n",
    "def _load_raw(topic: str, year: int) -> pd.DataFrame:\n",
    "    \"\"\"Load a raw ACS CSV for topic/year and return a slim df with category + estimate.\"\"\"\n",
    "    path = files_by_year[topic][year]\n",
    "    df = pd.read_csv(path)\n",
    "    est_col = _find_est_col(df)\n",
    "    cat_col = \"Label (Grouping)\" if \"Label (Grouping)\" in df.columns else \"Label\"\n",
    "    out = df[[cat_col, est_col]].rename(columns={cat_col:\"category\", est_col:\"estimate\"}).copy()\n",
    "    out[\"category_n\"] = out[\"category\"].map(_norm_label)\n",
    "    # make numeric; keep original estimate safe\n",
    "    out[\"estimate\"] = pd.to_numeric(out[\"estimate\"].astype(str).str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b2601b-1839-475f-ae0d-ecbb8e81e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_population(year: int) -> pd.DataFrame:\n",
    "    dfc = _load_raw(\"population\", year)\n",
    "    total = _pick_any(dfc, [\"total:\", \"total population\"])\n",
    "    return pd.DataFrame([{\n",
    "        \"topic\": \"population\",\n",
    "        \"year\": year,\n",
    "        \"population_total\": total\n",
    "    }])\n",
    "\n",
    "# run for all three years\n",
    "pop_clean = pd.concat([extract_population(y) for y in [2013, 2018, 2023]], ignore_index=True)\n",
    "pop_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a14c7dd-b0bb-41b8-86af-775fd8a3f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizes ACS population for the 5-year merge\n",
    "population_year = (\n",
    "    pop_clean.rename(columns={\"population_total\": \"population\"})[[\"year\",\"population\"]]\n",
    "             .assign(year=pd.to_numeric(lambda d: d[\"year\"], errors=\"coerce\"),\n",
    "                     population=pd.to_numeric(lambda d: d[\"population\"], errors=\"coerce\"))\n",
    "             .dropna(subset=[\"year\",\"population\"])\n",
    "             .assign(year=lambda d: d[\"year\"].round().astype(int))\n",
    "             .sort_values(\"year\")\n",
    "             .reset_index(drop=True)\n",
    ")\n",
    "display(population_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb6042-23e8-4815-8ca4-7cd8f9d0c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_housing(year: int) -> pd.DataFrame:\n",
    "    # housing units total\n",
    "    hous = _load_raw(\"housing\", year)\n",
    "    units = _pick_any(hous, [\"total housing units\", \"total:\"])\n",
    "\n",
    "    # occupied / owner / renter from tenure table\n",
    "    ten = _load_raw(\"tenure\", year)\n",
    "    occupied = _pick_any(ten, [\"total:\"])\n",
    "    owner    = _pick_any(ten, [\"owner occupied\", \"occupied housing units: owner occupied\", \"occupied housing units:\"])\n",
    "    renter   = _pick_any(ten, [\"renter occupied\", \"occupied housing units: renter occupied\"])\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        \"topic\": \"housing_tenure\",\n",
    "        \"year\": year,\n",
    "        \"housing_units_total\": units,\n",
    "        \"occupied_total\": occupied,\n",
    "        \"owner_occupied\": owner,\n",
    "        \"renter_occupied\": renter\n",
    "    }])\n",
    "\n",
    "housing_tenure_clean = pd.concat([extract_housing(y) for y in [2013, 2018, 2023]], ignore_index=True)\n",
    "housing_tenure_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a5ba5c-4776-4d85-b4f1-8c14e7893f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vehicles(year: int) -> pd.DataFrame:\n",
    "    veh = _load_raw(\"vehicles\", year)\n",
    "    total = _pick_any(veh, [\"total:\"])\n",
    "    v0 = _pick_any(veh, [\"no vehicle available\", \"no vehicles available\"])\n",
    "    v1 = _pick_any(veh, [\"1 vehicle available\"])\n",
    "    v2 = _pick_any(veh, [\"2 vehicles available\"])\n",
    "    v3p = _pick_any(veh, [\"3 or more vehicles available\", \"3 vehicles available\"])\n",
    "    return pd.DataFrame([{\n",
    "        \"topic\": \"vehicles\",\n",
    "        \"year\": year,\n",
    "        \"hh_total_for_veh\": total,\n",
    "        \"hh_0_veh\": v0,\n",
    "        \"hh_1_veh\": v1,\n",
    "        \"hh_2_veh\": v2,\n",
    "        \"hh_3plus_veh\": v3p\n",
    "    }])\n",
    "\n",
    "vehicles_clean = pd.concat([extract_vehicles(y) for y in [2013, 2018, 2023]], ignore_index=True)\n",
    "vehicles_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8dc9fc-8896-4616-bcda-9b5bdf226cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_commute_time(year: int) -> pd.DataFrame:\n",
    "    ct = _load_raw(\"commute_time\", year)\n",
    "    total = _pick_any(ct, [\"total:\"])\n",
    "    gt30 = sum([\n",
    "        _pick_any(ct, \"30 to 34 minutes\") or 0,\n",
    "        _pick_any(ct, \"35 to 39 minutes\") or 0,\n",
    "        _pick_any(ct, \"40 to 44 minutes\") or 0,\n",
    "        _pick_any(ct, \"45 to 59 minutes\") or 0,\n",
    "        _pick_any(ct, \"60 to 89 minutes\") or 0,\n",
    "        _pick_any(ct, \"90 or more minutes\") or 0,\n",
    "    ])\n",
    "    return pd.DataFrame([{\n",
    "        \"topic\": \"commute_time\",\n",
    "        \"year\": year,\n",
    "        \"workers_total\": total,\n",
    "        \"gt_30min_commute\": gt30\n",
    "    }])\n",
    "\n",
    "commute_clean = pd.concat([extract_commute_time(y) for y in [2013, 2018, 2023]], ignore_index=True)\n",
    "commute_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2310c10f-18d7-4444-bc12-eba5dc231629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transport_type(year: int) -> pd.DataFrame:\n",
    "    mode = _load_raw(\"transport_type\", year)\n",
    "    total = _pick_any(mode, [\"total:\"])\n",
    "    drive_alone = _pick_any(mode, [\"drove alone\"])\n",
    "    carpool = _pick_any(mode, [\"carpooled:\"])  # header has colon in your files\n",
    "    public_transport = _pick_any(mode, [\"public transportation (excluding taxicab):\"])\n",
    "    walked = _pick_any(mode, [\"walked\"])\n",
    "    bicycle = _pick_any(mode, [\"bicycle\"])\n",
    "    wfh = _pick_any(mode, [\"worked from home\"])\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        \"topic\": \"transport_type\",\n",
    "        \"year\": year,\n",
    "        \"mode_total\": total,\n",
    "        \"drive_alone\": drive_alone,\n",
    "        \"carpool\": carpool,\n",
    "        \"public_transport\": public_transport,\n",
    "        \"walked\": walked,\n",
    "        \"bicycle\": bicycle,\n",
    "        \"worked_from_home\": wfh\n",
    "    }])\n",
    "\n",
    "transport_clean = pd.concat([extract_transport_type(y) for y in [2013, 2018, 2023]], ignore_index=True)\n",
    "transport_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43f4d7-d564-4af5-af42-86f612774607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mode_labels(year):\n",
    "    mode = _load_raw(\"transport_type\", year)\n",
    "    print(f\"\\n--- transport_type labels {year} ---\")\n",
    "    for s in sorted(mode[\"category\"].dropna().unique())[:200]:\n",
    "        print(s)\n",
    "\n",
    "show_mode_labels(2013)\n",
    "show_mode_labels(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223fec3-49b7-4c76-b1bb-de9a89008677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transport_type(year: int) -> pd.DataFrame:\n",
    "    mode = _load_raw(\"transport_type\", year)\n",
    "\n",
    "    total             = _pick_any(mode, [\"total:\"])\n",
    "    drive_alone       = _pick_any(mode, [\"drove alone\"])\n",
    "    carpool           = _pick_any(mode, [\"carpooled:\", \"carpooled\"])  # with/without colon\n",
    "    public_transport  = _pick_any(mode, [\n",
    "        \"public transportation (excluding taxicab):\",\n",
    "        \"public transportation (excluding taxicab)\"\n",
    "    ])\n",
    "    walked            = _pick_any(mode, [\"walked\"])\n",
    "    bicycle           = _pick_any(mode, [\"bicycle\"])\n",
    "\n",
    "    # Handle pre-2020 wording and missing values:\n",
    "    wfh = _pick_any(mode, [\"worked from home\", \"worked at home\", \"work at home\"])\n",
    "    # If still None for earlier years, treat as 0 rather than NaN\n",
    "    if wfh is None:\n",
    "        wfh = 0.0\n",
    "\n",
    "    # Also default other missing subcategories to 0 so charts don’t break\n",
    "    for var in (\"drive_alone\",\"carpool\",\"public_transport\",\"walked\",\"bicycle\"):\n",
    "        if locals()[var] is None:\n",
    "            locals()[var] = 0.0\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        \"topic\": \"transport_type\",\n",
    "        \"year\": year,\n",
    "        \"mode_total\": total,\n",
    "        \"drive_alone\": drive_alone,\n",
    "        \"carpool\": carpool,\n",
    "        \"public_transport\": public_transport,\n",
    "        \"walked\": walked,\n",
    "        \"bicycle\": bicycle,\n",
    "        \"worked_from_home\": wfh\n",
    "    }])\n",
    "\n",
    "# re-run for all three years\n",
    "transport_clean = pd.concat([extract_transport_type(y) for y in [2013, 2018, 2023]], ignore_index=True)\n",
    "display(transport_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363cc292-89f6-4dba-b80a-2e6db9194ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chk = transport_clean.copy()\n",
    "chk[\"sub_sum\"] = chk[[\"drive_alone\",\"carpool\",\"public_transport\",\"walked\",\"bicycle\",\"worked_from_home\"]].sum(axis=1)\n",
    "print(chk[[\"year\",\"mode_total\",\"sub_sum\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847d01f-e884-4b03-bfe7-a61f169ddb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders\n",
    "ROOT = Path(\".\")\n",
    "FIG_DIR = ROOT / \"figures\"\n",
    "OUT_DIR = ROOT / \"outputs\"\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# clean default style\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b775f78-3bf8-4a7a-9bd8-10f326396600",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(\".\")\n",
    "FIG_DIR = ROOT / \"figures\"\n",
    "OUT_DIR = ROOT / \"outputs\"\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9182ecb5-f465-422a-8313-faa02567c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discovers a previously exported tidy file (common locations)\n",
    "CANDIDATE_TIDY = [\n",
    "    Path(\"aadt_davidson_long.csv\"),\n",
    "    Path(\"outputs/aadt_davidson_long.csv\"),\n",
    "    Path(\"out/aadt_davidson_long.csv\"),\n",
    "]\n",
    "TRAFFIC_TIDY_CSV = next((p for p in CANDIDATE_TIDY if p.exists()), None)\n",
    "\n",
    "# discovers a raw TDOT file by pattern if tidy is missing\n",
    "TRAFFIC_RAW_CSV = None if TRAFFIC_TIDY_CSV else next(iter(sorted(Path(\".\").glob(\"Traffic_Lines_*.csv\"))), None)\n",
    "\n",
    "def _guess_year_columns(df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"returns columns whose names contain a 4-digit year\"\"\"\n",
    "    cols = []\n",
    "    for c in df.columns:\n",
    "        if re.search(r\"(19|20)\\d{2}\", str(c)):\n",
    "            cols.append(c)\n",
    "    # preserves original order and uniqueness\n",
    "    seen, out = set(), []\n",
    "    for c in cols:\n",
    "        if c not in seen:\n",
    "            out.append(c); seen.add(c)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1905648-c2b3-4616-a14e-86392ff52756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _detect_long_columns(df: pd.DataFrame) -> tuple[str | None, str | None]:\n",
    "    \"\"\"returns (year_col, aadt_col) when data already stores one row per year\"\"\"\n",
    "    lower = {c: str(c).lower() for c in df.columns}\n",
    "    # finds a year column by name or by values\n",
    "    year_col = next((c for c, s in lower.items() if s == \"year\" or \"year\" in s), None)\n",
    "    if year_col is None:\n",
    "        # detects numeric-like year columns by values\n",
    "        for c in df.columns:\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            if s.notna().any():\n",
    "                vals = s.dropna()\n",
    "                if (vals.between(1990, 2035).mean() > 0.5):  # majority looks like years\n",
    "                    year_col = c\n",
    "                    break\n",
    "    # finds an AADT-like column\n",
    "    aadt_col = next((c for c, s in lower.items() if \"aadt\" in s), None)\n",
    "    if aadt_col is None:\n",
    "        # falls back to a volume/count-style numeric column\n",
    "        candidates = [c for c, s in lower.items() if any(k in s for k in [\"vol\", \"count\", \"aadt\", \"traffic\"])]\n",
    "        candidates = [c for c in candidates if pd.api.types.is_numeric_dtype(df[c]) or pd.to_numeric(df[c], errors=\"coerce\").notna().mean() > 0.8]\n",
    "        aadt_col = candidates[0] if candidates else None\n",
    "    return year_col, aadt_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca214ffc-7c20-4f27-b325-c04420ac7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tidy_from_wide(df: pd.DataFrame, id_col: str, keep_cols: tuple[str, ...] = ()) -> pd.DataFrame:\n",
    "    \"\"\"melts wide year columns into long format and extracts numeric year\"\"\"\n",
    "    year_cols = _guess_year_columns(df)\n",
    "    if not year_cols:\n",
    "        raise ValueError(\"No year-like columns found in traffic raw file.\")\n",
    "    out = df[list(keep_cols) + [id_col] + year_cols].melt(\n",
    "        id_vars=[id_col] + list(keep_cols),\n",
    "        value_vars=year_cols,\n",
    "        var_name=\"year_col\",\n",
    "        value_name=\"aadt\"\n",
    "    )\n",
    "    out[\"year\"] = pd.to_numeric(out[\"year_col\"].astype(str).str.extract(r\"((?:19|20)\\d{2})\")[0], errors=\"coerce\")\n",
    "    out = out.drop(columns=[\"year_col\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab361e4-26f2-4321-836c-8c2d7c124525",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAFFIC_TIDY_CSV and TRAFFIC_TIDY_CSV.exists():\n",
    "    traffic_long = pd.read_csv(TRAFFIC_TIDY_CSV)\n",
    "\n",
    "    # standardizes expected columns\n",
    "    if \"year\" not in traffic_long.columns:\n",
    "        yc = [c for c in traffic_long.columns if re.search(r\"(19|20)\\d{2}\", str(c))]\n",
    "        if yc:\n",
    "            traffic_long = traffic_long.rename(columns={yc[0]: \"year\"})\n",
    "    if \"aadt\" not in traffic_long.columns:\n",
    "        num_cols = traffic_long.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        num_cols = [c for c in num_cols if c != \"year\"]\n",
    "        if num_cols:\n",
    "            traffic_long = traffic_long.rename(columns={num_cols[0]: \"aadt\"})\n",
    "\n",
    "else:\n",
    "    if TRAFFIC_RAW_CSV is None:\n",
    "        raise FileNotFoundError(\"No tidy AADT file found and no raw 'Traffic_Lines_*.csv' present.\")\n",
    "\n",
    "    traffic_raw = pd.read_csv(TRAFFIC_RAW_CSV, low_memory=False)\n",
    "\n",
    "    # detects format\n",
    "    year_col, aadt_col = _detect_long_columns(traffic_raw)\n",
    "    wide_year_cols = _guess_year_columns(traffic_raw)\n",
    "\n",
    "    if year_col and aadt_col:\n",
    "        # selects columns by index to avoid duplicate-name DataFrame slices\n",
    "        def _first_index(df: pd.DataFrame, label) -> int | None:\n",
    "            matches = [i for i, c in enumerate(df.columns) if str(c) == str(label)]\n",
    "            return matches[0] if matches else None\n",
    "\n",
    "        yi = _first_index(traffic_raw, year_col)\n",
    "        ai = _first_index(traffic_raw, aadt_col)\n",
    "        if yi is None or ai is None:\n",
    "            raise ValueError(f\"Could not resolve indices for year='{year_col}' and aadt='{aadt_col}'.\")\n",
    "\n",
    "        # converts selected columns to numeric Series\n",
    "        year_s = pd.to_numeric(traffic_raw.iloc[:, yi], errors=\"coerce\")\n",
    "        aadt_s = pd.to_numeric(traffic_raw.iloc[:, ai], errors=\"coerce\")\n",
    "\n",
    "        # builds long-format frame and removes missing values\n",
    "        traffic_long = (\n",
    "            pd.DataFrame({\"year\": year_s, \"aadt\": aadt_s})\n",
    "            .dropna(subset=[\"year\", \"aadt\"])\n",
    "        )\n",
    "\n",
    "    elif wide_year_cols:\n",
    "        # treats file as wide; selects an id column when present\n",
    "        candidate_ids = [c for c in [\"STATION\",\"STATION_ID\",\"STATIONID\",\"SEGMENT_ID\",\"SEGMENTID\",\"GISID\",\"OBJECTID\",\"ID\"] if c in traffic_raw.columns]\n",
    "        id_col = candidate_ids[0] if candidate_ids else traffic_raw.columns[0]\n",
    "        traffic_long = _tidy_from_wide(traffic_raw, id_col=id_col, keep_cols=())\n",
    "        traffic_long = traffic_long.rename(columns={id_col: \"segment_id\"})\n",
    "\n",
    "    else:\n",
    "        # raises with diagnostics to show inspected columns\n",
    "        raise ValueError(\n",
    "            \"Traffic file does not expose year-like columns in headers and does not contain a clear YEAR/AADT pair.\\n\"\n",
    "            f\"Columns seen: {list(traffic_raw.columns)[:25]}...\"\n",
    "        )\n",
    "\n",
    "# aggregates to year level\n",
    "traffic_year = (\n",
    "    traffic_long\n",
    "    .dropna(subset=[\"year\", \"aadt\"])\n",
    "    .groupby(\"year\", as_index=False)[\"aadt\"].sum()\n",
    "    .rename(columns={\"aadt\": \"aadt_total\"})\n",
    "    .sort_values(\"year\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92eca2-9ec2-442b-a6ec-01cba9d6c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizes traffic to have ['year','aadt_total']\n",
    "if \"aadt_total\" not in traffic_year.columns:\n",
    "    if \"AADT\" in traffic_year.columns:\n",
    "        traffic_year_std = (\n",
    "            traffic_year.groupby(\"year\", as_index=False)[\"AADT\"].sum()\n",
    "                        .rename(columns={\"AADT\": \"aadt_total\"})\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"traffic_year must have 'aadt_total' or 'AADT' column.\")\n",
    "else:\n",
    "    traffic_year_std = traffic_year.copy()\n",
    "\n",
    "traffic_year_fix = (\n",
    "    traffic_year_std.assign(\n",
    "        year=pd.to_numeric(traffic_year_std[\"year\"], errors=\"coerce\"),\n",
    "        aadt_total=pd.to_numeric(traffic_year_std[\"aadt_total\"], errors=\"coerce\"),\n",
    "    )\n",
    "    .dropna(subset=[\"year\", \"aadt_total\"])\n",
    "    .assign(year=lambda d: d[\"year\"].round().astype(int))\n",
    "    .query(\"1990 <= year <= 2035\")\n",
    "    .drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
    "    .sort_values(\"year\")[[\"year\", \"aadt_total\"]]\n",
    ")\n",
    "\n",
    "# standardizes population to have ['year','population']\n",
    "if \"population\" not in population_year.columns:\n",
    "    if \"population_total\" in population_year.columns:\n",
    "        population_year_std = population_year.rename(columns={\"population_total\": \"population\"})\n",
    "    else:\n",
    "        raise ValueError(\"population_year must have 'population' (or 'population_total') column.\")\n",
    "else:\n",
    "    population_year_std = population_year.copy()\n",
    "\n",
    "population_year_fix = (\n",
    "    population_year_std.assign(\n",
    "        year=pd.to_numeric(population_year_std[\"year\"], errors=\"coerce\"),\n",
    "        population=pd.to_numeric(population_year_std[\"population\"], errors=\"coerce\"),\n",
    "    )\n",
    "    .dropna(subset=[\"year\", \"population\"])\n",
    "    .assign(year=lambda d: d[\"year\"].round().astype(int))\n",
    "    .query(\"1990 <= year <= 2035\")\n",
    "    .drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
    "    .sort_values(\"year\")[[\"year\", \"population\"]]\n",
    ")\n",
    "\n",
    "# builds year-indexed traffic frame and computes strict 5-year rolling mean\n",
    "ty = traffic_year_fix.set_index(\"year\").sort_index()\n",
    "ty[\"aadt_total_5y_mean\"] = ty[\"aadt_total\"].rolling(window=5, min_periods=5).mean()\n",
    "\n",
    "# prepares merge keys (ACS uses end-year label)\n",
    "traffic_5y = (\n",
    "    ty.reset_index()\n",
    "      .rename(columns={\"year\": \"end_year\"})\n",
    "      .loc[:, [\"end_year\", \"aadt_total_5y_mean\"]]\n",
    ")\n",
    "pop_5y = population_year_fix.rename(columns={\"year\": \"end_year\"})\n",
    "\n",
    "# merges and computes per-capita 5-year metric\n",
    "df5 = (\n",
    "    pop_5y.merge(traffic_5y, on=\"end_year\", how=\"left\")\n",
    "          .assign(\n",
    "              period=lambda d: d[\"end_year\"].apply(lambda y: f\"{y-4}–{y}\"),\n",
    "              aadt_per_capita_5y=lambda d: d[\"aadt_total_5y_mean\"] / d[\"population\"],\n",
    "          )\n",
    "          .dropna(subset=[\"aadt_total_5y_mean\", \"population\"])\n",
    "          .sort_values(\"end_year\")\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"traffic years:\", traffic_year_fix[\"year\"].min(), \"→\", traffic_year_fix[\"year\"].max(),\n",
    "    \"| population ACS end-years:\", population_year_fix[\"year\"].tolist(),\n",
    "    \"| rows after 5-year alignment:\", len(df5)\n",
    ")\n",
    "display(df5.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc00ad-f6b3-4c31-b08e-2ca11132f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads the year totals produced in Data Check and standardizes to traffic_year\n",
    "TRAFFIC_TOTALS_CSV = Path(\"outputs/aadt_davidson_year_totals.csv\")\n",
    "\n",
    "traffic_year = pd.read_csv(TRAFFIC_TOTALS_CSV)\n",
    "\n",
    "# normalizes column names just in case\n",
    "if \"AADT_YEAR\" in traffic_year.columns and \"year\" not in traffic_year.columns:\n",
    "    traffic_year = traffic_year.rename(columns={\"AADT_YEAR\": \"year\"})\n",
    "if \"aadt_total\" not in traffic_year.columns and \"AADT\" in traffic_year.columns:\n",
    "    traffic_year = (traffic_year.groupby(\"year\", as_index=False)[\"AADT\"]\n",
    "                               .sum()\n",
    "                               .rename(columns={\"AADT\": \"aadt_total\"}))\n",
    "\n",
    "# coerces types and sorts\n",
    "traffic_year[\"year\"] = pd.to_numeric(traffic_year[\"year\"], errors=\"coerce\")\n",
    "traffic_year[\"aadt_total\"] = pd.to_numeric(traffic_year[\"aadt_total\"], errors=\"coerce\")\n",
    "traffic_year = (traffic_year.dropna(subset=[\"year\",\"aadt_total\"])\n",
    "                             .assign(year=lambda d: d[\"year\"].round().astype(int))\n",
    "                             .sort_values(\"year\")\n",
    "                             .reset_index(drop=True))\n",
    "\n",
    "display(traffic_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4cb763-ab81-4208-a3e3-9b6bbd571dfd",
   "metadata": {},
   "source": [
    "YEAR_VALUE_RE = re.compile(r\"(?:19|20)\\d{2}\")  # matches a 4-digit year without capture groups\n",
    "\n",
    "def _first_estimate_column(df: pd.DataFrame) -> str:\n",
    "    \"\"\"returns the first column that ends with '!!Estimate' or contains 'Estimate'\"\"\"\n",
    "    for c in df.columns:\n",
    "        if isinstance(c, str) and c.endswith(\"!!Estimate\"):\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        if isinstance(c, str) and \"Estimate\" in c:\n",
    "            return c\n",
    "    raise ValueError(\"Estimate column not found (expects something like '...!!Estimate').\")\n",
    "\n",
    "def _to_number(x):\n",
    "    \"\"\"converts strings with commas/± to numeric\"\"\"\n",
    "    return pd.to_numeric(str(x).replace(\",\", \"\").replace(\"±\", \"\").strip(), errors=\"coerce\")\n",
    "\n",
    "def _load_population_years(root: Path = Path(\".\")) -> pd.DataFrame:\n",
    "    \"\"\"loads USCPOPDATA CSVs, extracts year and population, and aggregates to year\"\"\"\n",
    "    files = sorted(root.glob(\"USCPOPDATA*.csv\"))\n",
    "    if not files and (root / \"USCPOPDATA.csv\").exists():\n",
    "        files = [root / \"USCPOPDATA.csv\"]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No USCPOPDATA CSV found.\")\n",
    "\n",
    "    frames = []\n",
    "    for p in files:\n",
    "        df = pd.read_csv(p)\n",
    "\n",
    "        # identifies a year column by name or by values (uses non-capturing regex to avoid warnings)\n",
    "        year_col = next((c for c in df.columns if str(c).strip().lower() in {\"year\", \"yr\", \"time\"}), None)\n",
    "        if year_col is None:\n",
    "            for c in df.columns:\n",
    "                if df[c].astype(str).str.contains(YEAR_VALUE_RE, na=False).any():\n",
    "                    year_col = c\n",
    "                    break\n",
    "\n",
    "        est_col = _first_estimate_column(df)\n",
    "\n",
    "        # extracts year values from the detected column or, if absent, from the filename\n",
    "        if year_col is not None:\n",
    "            extracted_year = df[year_col].astype(str).str.extract(YEAR_VALUE_RE, expand=False)\n",
    "        else:\n",
    "            m = YEAR_VALUE_RE.search(p.name)\n",
    "            const_year = m.group(0) if m else np.nan\n",
    "            extracted_year = pd.Series([const_year] * len(df), index=df.index)\n",
    "\n",
    "        tmp = (\n",
    "            df.assign(\n",
    "                year=pd.to_numeric(extracted_year, errors=\"coerce\"),\n",
    "                population=lambda d: d[est_col].map(_to_number)\n",
    "            )\n",
    "            .dropna(subset=[\"year\", \"population\"])\n",
    "            .groupby(\"year\", as_index=False)[\"population\"].sum()\n",
    "        )\n",
    "        frames.append(tmp)\n",
    "\n",
    "    population_year = (\n",
    "        pd.concat(frames, ignore_index=True)\n",
    "        .groupby(\"year\", as_index=False)[\"population\"].sum()\n",
    "        .sort_values(\"year\")\n",
    "    )\n",
    "    return population_year\n",
    "\n",
    "population_year = _load_population_years()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce655c-ca5f-4224-a690-58dbcbc778ef",
   "metadata": {},
   "source": [
    "# standardizes traffic years and removes duplicates\n",
    "traffic_year_fix = (\n",
    "    traffic_year.assign(year=pd.to_numeric(traffic_year[\"year\"], errors=\"coerce\"))\n",
    "                .dropna(subset=[\"year\"])\n",
    "                .assign(year=lambda d: d[\"year\"].round().astype(int))\n",
    "                .query(\"1990 <= year <= 2035\")\n",
    "                .drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
    "                .sort_values(\"year\")\n",
    ")\n",
    "\n",
    "# standardizes population years (ACS 5-year end-years)\n",
    "population_year_fix = (\n",
    "    population_year.assign(year=pd.to_numeric(population_year[\"year\"], errors=\"coerce\"))\n",
    "                   .dropna(subset=[\"year\"])\n",
    "                   .assign(year=lambda d: d[\"year\"].round().astype(int))\n",
    "                   .query(\"1990 <= year <= 2035\")\n",
    "                   .drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
    "                   .sort_values(\"year\")\n",
    ")\n",
    "\n",
    "# computes traffic 5-year rolling mean ending at each year (uses >=3 years when edges are sparse)\n",
    "ty = traffic_year_fix.set_index(\"year\").sort_index()\n",
    "ty[\"aadt_total_5y_mean\"] = (\n",
    "    ty[\"aadt_total\"].rolling(window=5, min_periods=3).mean()\n",
    ")\n",
    "\n",
    "traffic_5y = (\n",
    "    ty.reset_index()\n",
    "      .rename(columns={\"year\": \"end_year\"})\n",
    "      [[\"end_year\", \"aadt_total_5y_mean\"]]\n",
    ")\n",
    "\n",
    "# prepares ACS 5-year table keyed by end_year\n",
    "pop_5y = population_year_fix.rename(columns={\"year\": \"end_year\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8b8a8-bf4d-4217-839a-89346bfbccae",
   "metadata": {},
   "source": [
    "# Merges ACS 5-year population with traffic 5-year rolling mean ===\n",
    "df5 = (\n",
    "    pop_5y.merge(traffic_5y, on=\"end_year\", how=\"left\")\n",
    "          .dropna(subset=[\"aadt_total_5y_mean\"])\n",
    "          .assign(\n",
    "              period=lambda d: d[\"end_year\"].apply(lambda y: f\"{y-4}–{y}\"),\n",
    "              aadt_per_capita_5y=lambda d: d[\"aadt_total_5y_mean\"] / d[\"population\"]\n",
    "          )\n",
    "          .sort_values(\"end_year\")\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Rows after 5-year alignment:\", len(df5))\n",
    "display(df5.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7951a-05ec-44f8-8705-4ca9634afa87",
   "metadata": {},
   "source": [
    "traffic_year_fix = (\n",
    "    traffic_year.assign(year=pd.to_numeric(traffic_year[\"year\"], errors=\"coerce\"),\n",
    "                        aadt_total=pd.to_numeric(traffic_year.get(\"aadt_total\", traffic_year.get(\"AADT\")), errors=\"coerce\"))\n",
    "                .dropna(subset=[\"year\",\"aadt_total\"])\n",
    "                .assign(year=lambda d: d[\"year\"].round().astype(int))\n",
    "                .query(\"1990 <= year <= 2035\")\n",
    "                .drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
    "                .sort_values(\"year\")[[\"year\",\"aadt_total\"]]\n",
    ")\n",
    "\n",
    "population_year_fix = (\n",
    "    population_year.assign(year=pd.to_numeric(population_year[\"year\"], errors=\"coerce\"),\n",
    "                           population=pd.to_numeric(population_year.get(\"population\"), errors=\"coerce\"))\n",
    "                   .dropna(subset=[\"year\",\"population\"])\n",
    "                   .assign(year=lambda d: d[\"year\"].round().astype(int))\n",
    "                   .query(\"1990 <= year <= 2035\")\n",
    "                   .drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
    "                   .sort_values(\"year\")[[\"year\",\"population\"]]\n",
    ")\n",
    "\n",
    "ty = traffic_year_fix.set_index(\"year\").sort_index()\n",
    "ty[\"aadt_total_5y_mean\"] = ty[\"aadt_total\"].rolling(window=5, min_periods=5).mean()\n",
    "\n",
    "traffic_5y = ty.reset_index().rename(columns={\"year\":\"end_year\"})[[\"end_year\",\"aadt_total_5y_mean\"]]\n",
    "pop_5y = population_year_fix.rename(columns={\"year\":\"end_year\"})\n",
    "\n",
    "df5 = (\n",
    "    pop_5y.merge(traffic_5y, on=\"end_year\", how=\"left\")\n",
    "          .assign(period=lambda d: d[\"end_year\"].apply(lambda y: f\"{y-4}–{y}\"),\n",
    "                  aadt_per_capita_5y=lambda d: d[\"aadt_total_5y_mean\"] / d[\"population\"])\n",
    "          .dropna(subset=[\"aadt_total_5y_mean\",\"population\"])\n",
    "          .sort_values(\"end_year\")\n",
    "          .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f684ccde-7bd0-4281-a1c6-f69e9265ba46",
   "metadata": {},
   "source": [
    "# Exports (ACS 5-year aligned): writes merged CSV and saves figures ===\n",
    "ROOT = Path(\".\")\n",
    "FIG_DIR = ROOT / \"figures\"\n",
    "OUT_DIR = ROOT / \"outputs\"\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# writes merged 5-year table\n",
    "out_csv_5y = OUT_DIR / \"traffic_population_merged_5y.csv\"\n",
    "df5.to_csv(out_csv_5y, index=False)\n",
    "\n",
    "# uses period labels for tick marks\n",
    "x = df5[\"end_year\"]\n",
    "xticks = df5[\"period\"]\n",
    "\n",
    "# population trend (ACS 5-year, labeled by period)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x, df5[\"population\"], marker=\"o\")\n",
    "plt.title(\"Population Trend (ACS 5-year)\")\n",
    "plt.xlabel(\"ACS Period End Year\")\n",
    "plt.ylabel(\"Population\")\n",
    "plt.xticks(x, xticks, rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"population_trend_5y.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# traffic trend (5-year rolling mean, labeled by period)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x, df5[\"aadt_total_5y_mean\"], marker=\"o\")\n",
    "plt.title(\"Total AADT (5-year rolling mean)\")\n",
    "plt.xlabel(\"ACS Period End Year\")\n",
    "plt.ylabel(\"Total AADT (5y mean)\")\n",
    "plt.xticks(x, xticks, rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"traffic_trend_5y.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# population vs. traffic (ACS 5-year vs 5-year mean AADT)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(df5[\"population\"], df5[\"aadt_total_5y_mean\"])\n",
    "plt.title(\"Traffic vs. Population (5-year aligned)\")\n",
    "plt.xlabel(\"Population (ACS 5-year)\")\n",
    "plt.ylabel(\"Total AADT (5y mean)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"pop_vs_traffic_scatter_5y.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# per-capita AADT (5-year aligned)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x, df5[\"aadt_per_capita_5y\"], marker=\"o\")\n",
    "plt.title(\"Per-Capita Traffic (AADT per person, 5-year aligned)\")\n",
    "plt.xlabel(\"ACS Period End Year\")\n",
    "plt.ylabel(\"AADT / Capita (5y mean)\")\n",
    "plt.xticks(x, xticks, rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"per_capita_congestion_5y.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c1dfc-49bc-4e09-b6e1-905f65274bf1",
   "metadata": {},
   "source": [
    "print(\"traffic_year shape:\", traffic_year.shape)\n",
    "print(traffic_year.head(3)); print(traffic_year.tail(3))\n",
    "\n",
    "print(\"\\npopulation_year shape:\", population_year.shape)\n",
    "print(population_year.head(3)); print(population_year.tail(3))\n",
    "\n",
    "years_traffic = pd.to_numeric(traffic_year[\"year\"], errors=\"coerce\").dropna().astype(int)\n",
    "years_pop     = pd.to_numeric(population_year[\"year\"], errors=\"coerce\").dropna().astype(int)\n",
    "print(\"\\ntraffic years:\", sorted(years_traffic.unique())[:20], \"...\", sorted(years_traffic.unique())[-5:])\n",
    "print(\"population years (ACS end-years):\", sorted(years_pop.unique()))\n",
    "print(\"exact overlap:\", sorted(set(years_traffic) & set(years_pop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6a37e-55e2-4c84-91c7-a1558ce04389",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_year_fix = (\n",
    "    traffic_year.assign(year=pd.to_numeric(traffic_year[\"year\"], errors=\"coerce\"))\n",
    "                .dropna(subset=[\"year\"])\n",
    "                .assign(year=lambda d: d[\"year\"].round().astype(int))\n",
    "                .query(\"1990 <= year <= 2035\")\n",
    "                .drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
    "                .sort_values(\"year\")\n",
    ")\n",
    "\n",
    "population_year_fix = (\n",
    "    population_year.assign(year=pd.to_numeric(population_year[\"year\"], errors=\"coerce\"))\n",
    "                   .dropna(subset=[\"year\"])\n",
    "                   .assign(year=lambda d: d[\"year\"].round().astype(int))\n",
    "                   .query(\"1990 <= year <= 2035\")\n",
    "                   .drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
    "                   .sort_values(\"year\")\n",
    ")\n",
    "\n",
    "# computes 5-year rolling mean for traffic \n",
    "ty[\"aadt_total_5y_mean\"] = ty[\"aadt_total\"].rolling(window=5, min_periods=5).mean()\n",
    "\n",
    "traffic_5y = (\n",
    "    ty.reset_index()\n",
    "      .rename(columns={\"year\": \"end_year\"})\n",
    "      [[\"end_year\", \"aadt_total_5y_mean\"]]\n",
    ")\n",
    "\n",
    "pop_5y = population_year_fix.rename(columns={\"year\": \"end_year\"})\n",
    "\n",
    "df5 = (\n",
    "    pop_5y.merge(traffic_5y, on=\"end_year\", how=\"left\")\n",
    "          .assign(\n",
    "              period=lambda d: d[\"end_year\"].apply(lambda y: f\"{y-4}–{y}\"),\n",
    "              aadt_per_capita_5y=lambda d: d[\"aadt_total_5y_mean\"] / d[\"population\"]\n",
    "          )\n",
    "          .dropna(subset=[\"aadt_total_5y_mean\", \"population\"])\n",
    "          .sort_values(\"end_year\")\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"rows after 5-year alignment:\", len(df5))\n",
    "display(df5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e132f4-b063-452d-b394-022045bea5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(\".\")\n",
    "FIG_DIR = ROOT / \"figures\"\n",
    "OUT_DIR = ROOT / \"outputs\"\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "if len(df5) == 0:\n",
    "    raise ValueError(\"No rows after 5-year alignment. Check the diagnostics printout to align years/columns.\")\n",
    "\n",
    "# writes merged 5-year table\n",
    "(df5[[\"end_year\",\"period\",\"population\",\"aadt_total_5y_mean\",\"aadt_per_capita_5y\"]]\n",
    " .to_csv(OUT_DIR / \"traffic_population_merged_5y.csv\", index=False))\n",
    "\n",
    "x = df5[\"end_year\"]; xticks = df5[\"period\"]\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(x, df5[\"population\"], marker=\"o\")\n",
    "plt.title(\"Population Trend (ACS 5-year)\")\n",
    "plt.xlabel(\"ACS Period End Year\"); plt.ylabel(\"Population\")\n",
    "plt.xticks(x, xticks, rotation=0)\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR/\"population_trend_5y.png\", dpi=300, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(x, df5[\"aadt_total_5y_mean\"], marker=\"o\")\n",
    "plt.title(\"Total AADT (5-year rolling mean)\")\n",
    "plt.xlabel(\"ACS Period End Year\"); plt.ylabel(\"Total AADT (5y mean)\")\n",
    "plt.xticks(x, xticks, rotation=0)\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR/\"traffic_trend_5y.png\", dpi=300, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(df5[\"population\"], df5[\"aadt_total_5y_mean\"])\n",
    "plt.title(\"Traffic vs. Population (5-year aligned)\")\n",
    "plt.xlabel(\"Population (ACS 5-year)\"); plt.ylabel(\"Total AADT (5y mean)\")\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR/\"pop_vs_traffic_scatter_5y.png\", dpi=300, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(x, df5[\"aadt_per_capita_5y\"], marker=\"o\")\n",
    "plt.title(\"Per-Capita Traffic (AADT per person, 5-year aligned)\")\n",
    "plt.xlabel(\"ACS Period End Year\"); plt.ylabel(\"AADT / Capita (5y mean)\")\n",
    "plt.xticks(x, xticks, rotation=0)\n",
    "plt.tight_layout(); plt.savefig(FIG_DIR/\"per_capita_congestion_5y.png\", dpi=300, bbox_inches=\"tight\"); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6019bc-2554-463f-a810-772902f6b6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b8e00-8475-488a-80a9-a12371965476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f4930-6a3a-4472-af59-c04b02eb94ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdfe2c1-d84b-4dc3-adc7-a6418f1dd526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4f483-5c19-4bbc-a908-536f50d0a7df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26a359-d2f5-49f4-a2ea-f04e20944e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e9908d-0045-4186-9321-b4953eb8673e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22afc9db-b03d-4783-8fec-db618d4e5d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b65d15-d3b8-4737-ba88-5a1a33cd53ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38394ff-d5c3-44b4-a9ef-091125fe46de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cd166e-a747-4b9b-8222-942f87eb1285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aab42b-1069-4204-ac68-f800b60f6436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047dac10-6d5e-4f2f-8cb9-1986d3d15b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ee303-2e17-4adc-a8c7-99772a6646b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9527f12-d4f9-4d29-824d-c0a12a4cc5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
